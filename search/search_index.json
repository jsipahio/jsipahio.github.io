{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"John Sipahioglu Instructor, Computer Science - Stark State College PhD Student, Kent State University Eventually, I will clean up this site to have all my info. For now, it has info sheets/\"eBooks\" for my courses.","title":"Home"},{"location":"#john-sipahioglu","text":"","title":"John Sipahioglu"},{"location":"#instructor-computer-science-stark-state-college","text":"","title":"Instructor, Computer Science - Stark State College"},{"location":"#phd-student-kent-state-university","text":"Eventually, I will clean up this site to have all my info. For now, it has info sheets/\"eBooks\" for my courses.","title":"PhD Student, Kent State University"},{"location":"CSE122/","text":"CSE122 This is mostly setup help for students who cannot, or do not wish to, use Visual Studio. Students on Mac (or Linux or Windows) can follow these instructions to install the .NET SDK, Visual Studio Code, and create and run .NET console apps from the command line. - Setup - This provides instructions to install .NET and VS Code, and test that they are working. - .NET Commands - This provides instructions for how to create your labs and open them in VS Code. - Terminal Basics - This provides instructions for navigating a terminal shell, if you would like to get more familiar with that. This one is mostly optional, but may be helpful if you get stuck with something in the terminal, or just want to learn more.","title":"Home"},{"location":"CSE122/#cse122","text":"This is mostly setup help for students who cannot, or do not wish to, use Visual Studio. Students on Mac (or Linux or Windows) can follow these instructions to install the .NET SDK, Visual Studio Code, and create and run .NET console apps from the command line. - Setup - This provides instructions to install .NET and VS Code, and test that they are working. - .NET Commands - This provides instructions for how to create your labs and open them in VS Code. - Terminal Basics - This provides instructions for navigating a terminal shell, if you would like to get more familiar with that. This one is mostly optional, but may be helpful if you get stuck with something in the terminal, or just want to learn more.","title":"CSE122"},{"location":"CSE122/dotnet_commands/","text":".NET Commands To create and run your Visual Basic applications, you will need to use the .NET command line interface (CLI). This is the bare minimum amount of terminal usage you will have to do for this class, if you are not using Visual Studio. First things first, you will need to open your project in VS Code. To create a new project, the first thing you should do is create a new folder for your project to go in. You can do this using your file explorer (Windows File Explorer, Mac Finder) application, or the terminal, if you are comfortable with it. Figure 1: New folder created for lab assignment Once you've created the folder, open VS Code and you will have the option to \"Open Folder...\" from the Welcome screen, or by clicking the \"File\" dropdown in the top-left corner and selecting \"Open Folder...\". Open folder locations in VS Code VS Code will open your file explorer application, and you can navigate to and select the folder you just created for your lab project. Make sure to select the lab folder, and not the one containing it. The bar at the bottom says \"Folder: week3-lab\", which is what we want. You do not want to open the \"CSE122\" folder This will open your folder in VS Code. It may ask if you trust the authors. Since you are the author, you should select \"Yes\". If you look at the file listing on the left side of the VS Code window, you may notice that it is empty. If you see a bunch of files, you probably selected the wrong folder. You should go to the File tab, select \"Close Folder\", then try opening your lab folder again. Empty Explorer in VS Code. If this is not open, click the icon that looks like a paper with the corner folded over that you see highlighted in this screenshot To create the project files, you should open the integrated terminal by holding the control (CTRL) key and pressing ~. The ~ key is to the left of the \"1\" key on your keyboard. The terminal will automatically be created in the folder that you have open in VS Code. In the terminal, type dotnet new console --language VB . This will create a subfolder called obj , a configuration file that has the same name as your folder with a vbproj extension, and a file called Program.vb . Program.vb contains the source code for your lab, and is the only file you actually have to interact with. You should see these files in the explorer on the left side of the VS Code window now: These are the default console app files for a Visual Basic project. Use the Program.vb file as your starting point for all your labs To test the lab, type dotnet run in the terminal. This will compile and run your lab. The message \"Hello World!\" should be printed to the terminal once it completes. Important .NET Commands To test that .NET is installed, run dotnet --list-sdks . It should print out a number and file path, such as \"10.0.100 [/usr/share/dotnet/sdk]\". If it doesn't, you will need to try reinstalling .NET. To create a new console application in Visual Basic, run the command dotnet new console --language vb in the folder you want to create the VB project in. You may want to double-check you are in the right folder by first running the command pwd , to print the current folder the terminal is accessing. To compile and run your lab, run the command dotnet run . When we write programs that have user input, you will enter your input into the same terminal you ran the lab from.","title":"Creating Labs"},{"location":"CSE122/dotnet_commands/#net-commands","text":"To create and run your Visual Basic applications, you will need to use the .NET command line interface (CLI). This is the bare minimum amount of terminal usage you will have to do for this class, if you are not using Visual Studio. First things first, you will need to open your project in VS Code. To create a new project, the first thing you should do is create a new folder for your project to go in. You can do this using your file explorer (Windows File Explorer, Mac Finder) application, or the terminal, if you are comfortable with it. Figure 1: New folder created for lab assignment Once you've created the folder, open VS Code and you will have the option to \"Open Folder...\" from the Welcome screen, or by clicking the \"File\" dropdown in the top-left corner and selecting \"Open Folder...\". Open folder locations in VS Code VS Code will open your file explorer application, and you can navigate to and select the folder you just created for your lab project. Make sure to select the lab folder, and not the one containing it. The bar at the bottom says \"Folder: week3-lab\", which is what we want. You do not want to open the \"CSE122\" folder This will open your folder in VS Code. It may ask if you trust the authors. Since you are the author, you should select \"Yes\". If you look at the file listing on the left side of the VS Code window, you may notice that it is empty. If you see a bunch of files, you probably selected the wrong folder. You should go to the File tab, select \"Close Folder\", then try opening your lab folder again. Empty Explorer in VS Code. If this is not open, click the icon that looks like a paper with the corner folded over that you see highlighted in this screenshot To create the project files, you should open the integrated terminal by holding the control (CTRL) key and pressing ~. The ~ key is to the left of the \"1\" key on your keyboard. The terminal will automatically be created in the folder that you have open in VS Code. In the terminal, type dotnet new console --language VB . This will create a subfolder called obj , a configuration file that has the same name as your folder with a vbproj extension, and a file called Program.vb . Program.vb contains the source code for your lab, and is the only file you actually have to interact with. You should see these files in the explorer on the left side of the VS Code window now: These are the default console app files for a Visual Basic project. Use the Program.vb file as your starting point for all your labs To test the lab, type dotnet run in the terminal. This will compile and run your lab. The message \"Hello World!\" should be printed to the terminal once it completes.","title":".NET Commands"},{"location":"CSE122/dotnet_commands/#important-net-commands","text":"To test that .NET is installed, run dotnet --list-sdks . It should print out a number and file path, such as \"10.0.100 [/usr/share/dotnet/sdk]\". If it doesn't, you will need to try reinstalling .NET. To create a new console application in Visual Basic, run the command dotnet new console --language vb in the folder you want to create the VB project in. You may want to double-check you are in the right folder by first running the command pwd , to print the current folder the terminal is accessing. To compile and run your lab, run the command dotnet run . When we write programs that have user input, you will enter your input into the same terminal you ran the lab from.","title":"Important .NET Commands"},{"location":"CSE122/setup/","text":"Setup Help This guide aims to help students who cannot, or wish not to, use Visual Studio for CSE122, programming logic and problem solving. This guide is applicable to all operating systems, with some minor variations. As an alternative to Visual Studio, you may use Visual Studio Code (commonly referred to as VS Code, or just Code) and manually install the .NET SDK (Software Development). You will have to learn to operate a terminal a bit to use this method, but I think this is a good skill to have. Step 1: Install VS Code If you go to this link (https://code.visualstudio.com/), the website should (as long as you don't have a browser/settings that hide it) detect your operating system and processor. There will be a Download button at the top of page, with the name of your OS. If the OS is wrong, or if you don't see the Download button, go here (https://code.visualstudio.com/Download) and select the Download that matches your OS and CPU. It will likely take a minute or two to download. Once it finishes, run the installer. The default settings for the install will be fine. Once finished, verify that you can open VS Code. Once you've checked it, go ahead and close it while we install the .NET SDK. Step 2: Install the .NET SDK The .NET SDK installs the dotnet command line tool, along with the compilers and project templates for .NET languages (C#, F#, and Visual Basic). This link (https://dotnet.microsoft.com/en-us/download) should automatically detect your OS/CPU architecture and provide the correct installer download. If it does not, go here (https://dotnet.microsoft.com/en-us/download/dotnet/10.0) and find the version that matches your OS. Windows users with Intel or AMD processors should choose Windows x64. Windows users with a Snapdragon processor should choose Windows Arm64. Mac users should choose Mac x64 if they have an Intel CPU, or Arm64 if they have an Apple Silicon (M1/2/3/4). Once you've downloaded the installer (again, it may take a couple minutes), run it and click through until you get the option to install. Step 3: Verify Installation Open VS Code. You can open an integrated terminal session within VS Code by holding the CTRL key and pressing \"~\" (the key to the left of \"1\"). You should get a terminal at the bottom of your screen, similar to what is shown below: In it, type the following command: dotnet --list-sdks You should get output similar to the following: Let's create a new directory to create a test project in. In your terminal, type: mkdir vb-test This creates a directory (aka folder) called \"vb-test\". Now, let's enter that directory, using the cd command. cd is short for change directory. cd vb-test Now, we will create a new Visual Basic console application in this directory using the command below: dotnet new console --language vb This will automatically generate the files required to create and run a Visual Basic Console application. Running ls -la in the terminal will display the files. You want to see something similar to the output below: The single period represents the current directory, and the .. is for the parent directory. We don't really care about those. What are important are the Program.vb , obj and vb-test.vbproj . The Program.vb file contains the source code for your program. The obj directory contains some generated files required by .NET. The vb-test.vbproj files are instructions for the .NET compiler. If these files are all present, you can try building and running you project with the following command. This should print \"Hello World!\" to the terminal: dotnet run Expected output: If you've gotten to this point, congratulations! Your installation of VS Code and .NET have been successful. Step 4: Setting Up for the Rest of the Semester Now that you have your installation verified, I'd recommend getting a folder set up for the rest of the semester. Either using the terminal, or your file explorer GUI, create a folder for this class wherever you'd like. We have 6 Visual Basic labs in this class, in Weeks 3, 5, 8, 10, 12, and 14. These correspond to chapters 2, 3, 4, 5, 6, and 9 from the book. Make a folder for each, using either the week or chapter numbers to keep them separate. Now, when the time comes, you can go to \"File in the top right corner of VS Code, select \"Open folder\", and navigate to the correct lab folder for the week we are on. Then, you can open a terminal in VS Code using CTRL + ~, and running dotnet new console --language vb . You can edit your Program.vb source code file by clicking on it in the list of files on the left side of the VS Code window. To test your code, run dotnet run in the terminal. Step 5: Check the Terminal Basics Doc I've also included a \"Terminal Basics\" sheet under this class. This will provide you with some basics of using a command line interface. I've tried to do this in a way that avoids the terminal as much as possible, but there are instances where it is needed. Troubleshooting As the semester goes on, I'll add issues that people encounter to this troubleshooting guide, so you can check back here if you have any issues. Issue: When running dotnet --list-sdks , I get a message to the effect of \"dotnet is not a valid command\". Solution: This means that the dotnet tool is not installed, or did not get added to your system's path. Follow the instructions under \"Step 2: Install the .NET SDK\". If you are sure that it is installed correctly, the issue is likely that the dotnet command line tool did not get added to your system's path. Windows - In the search box in the task bar, search \"environment variables\". If you are on a computer only you use, you can edit the System or \"Your Account\" variables. If it is a shared computer, edit the variables for your account. - In the top of the box, click Path to highlight it, and click the \"Edit...\" button. - A new box should pop up. Click the \"New\" button. This will allow you to type into the next unused row. - The default path for dotnet is \"C:\\Program Files\\dotnet\". Enter that into the box Mac - The only way this would happen is if you installed .NET to a non-standard location. I'd recommend running the installer again, uninstalling, and installing it again. If you still have issues, contact me. All - After following the steps for your OS, attempt to perform step 3 again to verify the installation. Issue: When I try running dotnet run , I get this message: \"Couldn't find a project to run. Ensure a project exists in /workspaces/jsipahio.github.io, or pass the path to the project using --project.\" Solution: This indicates that your terminal is not in the same directory as your project. If you are not comfortable using a terminal to navigate your file system, navigate to your project folder using your file explorer application. Make sure you are in the folder, you should see your Program.vb and .vbproj files. Copy the path by right-clicking the path in the top bar. Select \"Copy as text\" if you are on Windows. Mac users should hold the Option key while right-clicking, and select \"Copy name_of_folder As Pathname\". Go back to the terminal, type cd , add a space, then paste in the path you just copied. Hit enter. This should change your terminal's directory into the correct one. Try running dotnet run again.","title":"Setup Help"},{"location":"CSE122/setup/#setup-help","text":"This guide aims to help students who cannot, or wish not to, use Visual Studio for CSE122, programming logic and problem solving. This guide is applicable to all operating systems, with some minor variations. As an alternative to Visual Studio, you may use Visual Studio Code (commonly referred to as VS Code, or just Code) and manually install the .NET SDK (Software Development). You will have to learn to operate a terminal a bit to use this method, but I think this is a good skill to have.","title":"Setup Help"},{"location":"CSE122/setup/#step-1-install-vs-code","text":"If you go to this link (https://code.visualstudio.com/), the website should (as long as you don't have a browser/settings that hide it) detect your operating system and processor. There will be a Download button at the top of page, with the name of your OS. If the OS is wrong, or if you don't see the Download button, go here (https://code.visualstudio.com/Download) and select the Download that matches your OS and CPU. It will likely take a minute or two to download. Once it finishes, run the installer. The default settings for the install will be fine. Once finished, verify that you can open VS Code. Once you've checked it, go ahead and close it while we install the .NET SDK.","title":"Step 1: Install VS Code"},{"location":"CSE122/setup/#step-2-install-the-net-sdk","text":"The .NET SDK installs the dotnet command line tool, along with the compilers and project templates for .NET languages (C#, F#, and Visual Basic). This link (https://dotnet.microsoft.com/en-us/download) should automatically detect your OS/CPU architecture and provide the correct installer download. If it does not, go here (https://dotnet.microsoft.com/en-us/download/dotnet/10.0) and find the version that matches your OS. Windows users with Intel or AMD processors should choose Windows x64. Windows users with a Snapdragon processor should choose Windows Arm64. Mac users should choose Mac x64 if they have an Intel CPU, or Arm64 if they have an Apple Silicon (M1/2/3/4). Once you've downloaded the installer (again, it may take a couple minutes), run it and click through until you get the option to install.","title":"Step 2: Install the .NET SDK"},{"location":"CSE122/setup/#step-3-verify-installation","text":"Open VS Code. You can open an integrated terminal session within VS Code by holding the CTRL key and pressing \"~\" (the key to the left of \"1\"). You should get a terminal at the bottom of your screen, similar to what is shown below: In it, type the following command: dotnet --list-sdks You should get output similar to the following: Let's create a new directory to create a test project in. In your terminal, type: mkdir vb-test This creates a directory (aka folder) called \"vb-test\". Now, let's enter that directory, using the cd command. cd is short for change directory. cd vb-test Now, we will create a new Visual Basic console application in this directory using the command below: dotnet new console --language vb This will automatically generate the files required to create and run a Visual Basic Console application. Running ls -la in the terminal will display the files. You want to see something similar to the output below: The single period represents the current directory, and the .. is for the parent directory. We don't really care about those. What are important are the Program.vb , obj and vb-test.vbproj . The Program.vb file contains the source code for your program. The obj directory contains some generated files required by .NET. The vb-test.vbproj files are instructions for the .NET compiler. If these files are all present, you can try building and running you project with the following command. This should print \"Hello World!\" to the terminal: dotnet run Expected output: If you've gotten to this point, congratulations! Your installation of VS Code and .NET have been successful.","title":"Step 3: Verify Installation"},{"location":"CSE122/setup/#step-4-setting-up-for-the-rest-of-the-semester","text":"Now that you have your installation verified, I'd recommend getting a folder set up for the rest of the semester. Either using the terminal, or your file explorer GUI, create a folder for this class wherever you'd like. We have 6 Visual Basic labs in this class, in Weeks 3, 5, 8, 10, 12, and 14. These correspond to chapters 2, 3, 4, 5, 6, and 9 from the book. Make a folder for each, using either the week or chapter numbers to keep them separate. Now, when the time comes, you can go to \"File in the top right corner of VS Code, select \"Open folder\", and navigate to the correct lab folder for the week we are on. Then, you can open a terminal in VS Code using CTRL + ~, and running dotnet new console --language vb . You can edit your Program.vb source code file by clicking on it in the list of files on the left side of the VS Code window. To test your code, run dotnet run in the terminal.","title":"Step 4: Setting Up for the Rest of the Semester"},{"location":"CSE122/setup/#step-5-check-the-terminal-basics-doc","text":"I've also included a \"Terminal Basics\" sheet under this class. This will provide you with some basics of using a command line interface. I've tried to do this in a way that avoids the terminal as much as possible, but there are instances where it is needed.","title":"Step 5: Check the Terminal Basics Doc"},{"location":"CSE122/setup/#troubleshooting","text":"As the semester goes on, I'll add issues that people encounter to this troubleshooting guide, so you can check back here if you have any issues. Issue: When running dotnet --list-sdks , I get a message to the effect of \"dotnet is not a valid command\". Solution: This means that the dotnet tool is not installed, or did not get added to your system's path. Follow the instructions under \"Step 2: Install the .NET SDK\". If you are sure that it is installed correctly, the issue is likely that the dotnet command line tool did not get added to your system's path. Windows - In the search box in the task bar, search \"environment variables\". If you are on a computer only you use, you can edit the System or \"Your Account\" variables. If it is a shared computer, edit the variables for your account. - In the top of the box, click Path to highlight it, and click the \"Edit...\" button. - A new box should pop up. Click the \"New\" button. This will allow you to type into the next unused row. - The default path for dotnet is \"C:\\Program Files\\dotnet\". Enter that into the box Mac - The only way this would happen is if you installed .NET to a non-standard location. I'd recommend running the installer again, uninstalling, and installing it again. If you still have issues, contact me. All - After following the steps for your OS, attempt to perform step 3 again to verify the installation. Issue: When I try running dotnet run , I get this message: \"Couldn't find a project to run. Ensure a project exists in /workspaces/jsipahio.github.io, or pass the path to the project using --project.\" Solution: This indicates that your terminal is not in the same directory as your project. If you are not comfortable using a terminal to navigate your file system, navigate to your project folder using your file explorer application. Make sure you are in the folder, you should see your Program.vb and .vbproj files. Copy the path by right-clicking the path in the top bar. Select \"Copy as text\" if you are on Windows. Mac users should hold the Option key while right-clicking, and select \"Copy name_of_folder As Pathname\". Go back to the terminal, type cd , add a space, then paste in the path you just copied. Hit enter. This should change your terminal's directory into the correct one. Try running dotnet run again.","title":"Troubleshooting"},{"location":"CSE122/terminal_basics/","text":"Terminal Basics This fact sheet will help familiarize you with the terminal shell. In Linux and Mac, the application to access the shell is typically called \"Terminal\". In Windows, you can use Command Prompt (cmd.exe) or PowerShell (powershell.exe). You can also open a terminal in VS Code by holding the control (CTRL) key and pressing ~, which is to the left of your \"1\" key. On Windows, VS Code opens PowerShell by default, which I would recommend using. Command Cheat Sheet Here is a list of commonly used commands in the terminal shell. Note that these are for Mac/Linux, although PowerShell aliases many of its commands to match these. The commands for Command Prompt are totally different, in many cases. man (manual) - this command will print the help manual for whatever command/program you pass it, assuming that a manual is available for it. The manual can be navigated using PgUp and PgDn , and exited by pressing q on your keyboard. (On most modern Linux systems you can also scroll with the mouse) pwd (print working directory) - this command prints your current working directory in the file system. It is important to be aware of the directory you are in, and how to navigate the file system, so you are working in the correct location ls (list) - this command shows you the directory listing, i.e., the list of all the files in and subdirectories in the directory ls is querying. Below are some of the command switches -l - this switch changes the output to be in a more detailed list that shows you permissions, last access/modify date and time, and the size of file -a - this switch shows hidden files and directories. In Linux, directories that start with a period are hidden -h - this switch converts the bytes of a file's size into a more human-readable format, like kilobytes, megabytes, or gigabytes, depending on the size of the file The switches can be combined like the following: ls -lah (the order of the switches is not important, -ahl would do the same thing) By default, ls shows the listing for the current directory. But, you can pass it another directory as an argument: ls -l /var/log cd (change directory) - this command is used to change your working directory. It takes one argument, the directory you want to chage to. mkdir (make directory) - this command is used to create a new directory. -p - this switch is used to create intermediate directories if they don't exist cp (copy) - this command is used to copy files or directories from a source to a destination. Below are some common switches: -b - backup destination files -r - recursively copy, this is used to copy directories and subdirectories. By default, directories are skipped without this flag -f - force. If you try to copy over a file and that file is in use, it will attempt to force delete the destination file to complete the copy action -i - interactive. This will prompt you to answer yes or no before overwriting a file. By default, existing files are overwritten without warning -n - no-clobber. Blocks existing files from being overwritten mv (move) - this command will move a file from one location to another. Also, renaming a file is technically a \"move\" operation, so mv can be used to rename a file as well. -b - backup destination files -f - force. Always overwrite destination -i - interactive. Prompt before overwrite -n - no clobber. Never overwrite destination -u - update. Only update destination if it is older than the source or does not exist 'rm (remove) - this command will delete files and directories. Note that this will delete files permanently (there is not a recycle bin when using rm ) - -f - ignore if file is missing (in PowerShell, use -Force instead) - -i - prompt before each removal - -r - remove directories and their children recursively - -d - remove empty directories Understanding the Command Line Environment When using the terminal, you are interacting with your computer's filesystem using a text based interface. You are hopefully somewhat familiar with navigating your file system using your file explorer application provided by the operating system (File Explorer on Windows, Finder on Mac, Files on Ubuntu Linux, etc.). When you click your file explorer application, you get a GUI window that shows icons for the various files and folders you have. These files and folders themselves are contained within a folder in your file system. By default, Windows File Explorer opens yp to a Favorites and Recently accessed files screen, which is not an actual folder. Rather, it is a collection of folders. On Mac, Finder opens up your home directory, which contains your Desktop, Documents, iTunes, etc. folders. If you open a blank window in VS Code and open a terminal (using CTRL + ~), the terminal opens you your user directory (remember, directories in the terminal correspond to folders in file explorer). On Mac, your user directory is likely /Users/<your-username> . On Windows, it is typically C:\\Users\\<your-username> . This folder contains your Desktop, Documents, Downloads, etc. folders. You can check this by running the ls command. This command lists the files and directories contained in the current directory. To check the name of the directory you are currently in, you can use the pwd (print working directory) command. So, what if I am in /Users/john/ , and want to get to my Desktop folder? I can use the cd (change directory) command to do so. If I type cd Desktop , I will change directory to the Desktop folder within the current directory, /Users/john, that I am in. Now, if I run pwd , I will see that I am in /Users/john/Desktop/ . If I run ls , I will see all the files and folders that are contained within my Desktop folder. You can verify this by checking your own Desktop folder versus the list of files you get running ls . Your computer's directory has a tree-like structure. It starts with a root, which is represented by a / on Mac and C:\\ on Windows. Below is the typical, default directory structure of a Windows and Mac filesystem: Note: your computer may vary slightly from this, that is okay Windows C:\\ Program Files Program Files (x86) Users Administrator Guest <your-username> Desktop Documents Photos Downloads Windows Mac / bin boot dev etc lib home media mnt proc opt proc run srv sys usr Users <your-username> Desktop Documents Downloads iTunes var For the sake of this class, you only need to worry about the folders and files that are \"branches\" and \"leaves\" of directory that is your username, which is why I have only \"expanded\" those directories in the diagrams above. Returning to the example from a couple paragraphs ago, I have changed directory into /Users/john/Desktop/. But, what if I want to go back a directory? Terminal shells reserve two periods, .. , for this purpose. If I run the command cd .. , it will change my directory from /Users/john/Desktop/ to /Users/john/. If I run cd .. again, I will change directory from /Users/john/ to /Users/. If I wanted to get back to my Desktop folder, I would first have to cd john to get back to /Users/john/, then cd Desktop to get to /Users/john/Desktop/. Alternatively, I could provide the whole path to my Desktop folder, with the command cd john/Desktop . This would let me change from /Users/ to /Users/john/Desktop/ in a single command. As an example, let's say that this is what my (simplified) directory structure looks like: / Users john Desktop CSE122 lab1 lab1.vbproj obj Program.vb lab2 NET120 Documents Downloads Assume I am in a terminal, and run the command pwd . The output I get is /Users/john/Downloads/ . There are two things I want to accomplish. First, I want to get to my \"lab1\" and run the program. Secondly, I want to create the Visual Basic console app project for my \"lab2\". If we look at the directory tree, we can see that both lab1 and lab2 are directories contained within the directory \"CSE122\". \"CSE122\" is contained in my \"Desktop\" directory, which is under my name. Currently, I am in /Users/john/Downloads/. \"Desktop\" is directly below \"john\", so I first want to get to \"/Users/john/\". To do so, I can use cd .. to move up one directory. Running this command will change my location from /Users/john/Downloads/ to /Users/john/ . Now, I want to get to my \"Desktop\" folder. Since it is directly below \"john\" in the directory tree, I can use cd Desktop to get there. If I run pwd , my current directory is now /Users/john/Desktop/ . However, lab1 and lab2 are in \"CSE122\", not Desktop. \"CSE122\" is directly below Desktop, so to get there I can use cd CSE122 . I am now in /Users/john/Desktop/CSE122 . The first thing I wanted to do was to run my lab1 project. To do so, I should be in the \"lab1\" directory. Once again, I will use cd to get there. cd lab1 will put me in /Users/john/Desktop/CSE122/lab1/ . From here, I can run the command dotnet run to compile and execute my lab1 project. Next, I wanted to create a new project for lab2. The \"lab2\" directory is directly below \"CSE122\", and is not contained within \"lab1\" (use the indention to determine the level of a directory/file). Therefore, to get to \"lab2\", I must first use cd .. to exit the \"lab1\" directory and return to /Users/john/Desktop/CSE122 . Now, I can use cd lab2 to enter the directory /Users/john/Desktop/CSE122/lab2 . To create a new Visual Basic console app project, type the command dotnet new console --language VB . This will create the starter files required for a basic Visual Basic console app. If you run the ls command in the \"lab2\" directory, you will see that it has created a new directory called obj , a file called Program.vb , and a file called lab2.vbproj . Below is an example of the terminal I/O for this example. Lines that start with > are user input lines. Lines that start with # are comments, and are not part of the actual terminal I/O. Lines that do not start with any special characters are the terminal output. # checking my current directory > pwd /Users/john/Downloads # changing directory to my /User/john home directory > cd .. # displaying file listing > ls Desktop Documents Downloads # changing to Desktop and checking contents > cd Desktop > ls CSE122 NET120 # changing to CSE122 and checking contents > cd CSE122 > ls lab1 lab2 # changing to lab1 and running project > cd lab1 > dotnet run Hello World! # moving back a directory so I can change to lab2 > cd .. > cd lab2 # creating a new VB console app in lab2 > dotnet new console --language vb # there is a lot of text output when this happens that is not included here > ls lab2.vbproj obj Program.vb > dotnet run Hello World! >","title":"Terminal Basics"},{"location":"CSE122/terminal_basics/#terminal-basics","text":"This fact sheet will help familiarize you with the terminal shell. In Linux and Mac, the application to access the shell is typically called \"Terminal\". In Windows, you can use Command Prompt (cmd.exe) or PowerShell (powershell.exe). You can also open a terminal in VS Code by holding the control (CTRL) key and pressing ~, which is to the left of your \"1\" key. On Windows, VS Code opens PowerShell by default, which I would recommend using.","title":"Terminal Basics"},{"location":"CSE122/terminal_basics/#command-cheat-sheet","text":"Here is a list of commonly used commands in the terminal shell. Note that these are for Mac/Linux, although PowerShell aliases many of its commands to match these. The commands for Command Prompt are totally different, in many cases. man (manual) - this command will print the help manual for whatever command/program you pass it, assuming that a manual is available for it. The manual can be navigated using PgUp and PgDn , and exited by pressing q on your keyboard. (On most modern Linux systems you can also scroll with the mouse) pwd (print working directory) - this command prints your current working directory in the file system. It is important to be aware of the directory you are in, and how to navigate the file system, so you are working in the correct location ls (list) - this command shows you the directory listing, i.e., the list of all the files in and subdirectories in the directory ls is querying. Below are some of the command switches -l - this switch changes the output to be in a more detailed list that shows you permissions, last access/modify date and time, and the size of file -a - this switch shows hidden files and directories. In Linux, directories that start with a period are hidden -h - this switch converts the bytes of a file's size into a more human-readable format, like kilobytes, megabytes, or gigabytes, depending on the size of the file The switches can be combined like the following: ls -lah (the order of the switches is not important, -ahl would do the same thing) By default, ls shows the listing for the current directory. But, you can pass it another directory as an argument: ls -l /var/log cd (change directory) - this command is used to change your working directory. It takes one argument, the directory you want to chage to. mkdir (make directory) - this command is used to create a new directory. -p - this switch is used to create intermediate directories if they don't exist cp (copy) - this command is used to copy files or directories from a source to a destination. Below are some common switches: -b - backup destination files -r - recursively copy, this is used to copy directories and subdirectories. By default, directories are skipped without this flag -f - force. If you try to copy over a file and that file is in use, it will attempt to force delete the destination file to complete the copy action -i - interactive. This will prompt you to answer yes or no before overwriting a file. By default, existing files are overwritten without warning -n - no-clobber. Blocks existing files from being overwritten mv (move) - this command will move a file from one location to another. Also, renaming a file is technically a \"move\" operation, so mv can be used to rename a file as well. -b - backup destination files -f - force. Always overwrite destination -i - interactive. Prompt before overwrite -n - no clobber. Never overwrite destination -u - update. Only update destination if it is older than the source or does not exist 'rm (remove) - this command will delete files and directories. Note that this will delete files permanently (there is not a recycle bin when using rm ) - -f - ignore if file is missing (in PowerShell, use -Force instead) - -i - prompt before each removal - -r - remove directories and their children recursively - -d - remove empty directories","title":"Command Cheat Sheet"},{"location":"CSE122/terminal_basics/#understanding-the-command-line-environment","text":"When using the terminal, you are interacting with your computer's filesystem using a text based interface. You are hopefully somewhat familiar with navigating your file system using your file explorer application provided by the operating system (File Explorer on Windows, Finder on Mac, Files on Ubuntu Linux, etc.). When you click your file explorer application, you get a GUI window that shows icons for the various files and folders you have. These files and folders themselves are contained within a folder in your file system. By default, Windows File Explorer opens yp to a Favorites and Recently accessed files screen, which is not an actual folder. Rather, it is a collection of folders. On Mac, Finder opens up your home directory, which contains your Desktop, Documents, iTunes, etc. folders. If you open a blank window in VS Code and open a terminal (using CTRL + ~), the terminal opens you your user directory (remember, directories in the terminal correspond to folders in file explorer). On Mac, your user directory is likely /Users/<your-username> . On Windows, it is typically C:\\Users\\<your-username> . This folder contains your Desktop, Documents, Downloads, etc. folders. You can check this by running the ls command. This command lists the files and directories contained in the current directory. To check the name of the directory you are currently in, you can use the pwd (print working directory) command. So, what if I am in /Users/john/ , and want to get to my Desktop folder? I can use the cd (change directory) command to do so. If I type cd Desktop , I will change directory to the Desktop folder within the current directory, /Users/john, that I am in. Now, if I run pwd , I will see that I am in /Users/john/Desktop/ . If I run ls , I will see all the files and folders that are contained within my Desktop folder. You can verify this by checking your own Desktop folder versus the list of files you get running ls . Your computer's directory has a tree-like structure. It starts with a root, which is represented by a / on Mac and C:\\ on Windows. Below is the typical, default directory structure of a Windows and Mac filesystem: Note: your computer may vary slightly from this, that is okay Windows C:\\ Program Files Program Files (x86) Users Administrator Guest <your-username> Desktop Documents Photos Downloads Windows Mac / bin boot dev etc lib home media mnt proc opt proc run srv sys usr Users <your-username> Desktop Documents Downloads iTunes var For the sake of this class, you only need to worry about the folders and files that are \"branches\" and \"leaves\" of directory that is your username, which is why I have only \"expanded\" those directories in the diagrams above. Returning to the example from a couple paragraphs ago, I have changed directory into /Users/john/Desktop/. But, what if I want to go back a directory? Terminal shells reserve two periods, .. , for this purpose. If I run the command cd .. , it will change my directory from /Users/john/Desktop/ to /Users/john/. If I run cd .. again, I will change directory from /Users/john/ to /Users/. If I wanted to get back to my Desktop folder, I would first have to cd john to get back to /Users/john/, then cd Desktop to get to /Users/john/Desktop/. Alternatively, I could provide the whole path to my Desktop folder, with the command cd john/Desktop . This would let me change from /Users/ to /Users/john/Desktop/ in a single command. As an example, let's say that this is what my (simplified) directory structure looks like: / Users john Desktop CSE122 lab1 lab1.vbproj obj Program.vb lab2 NET120 Documents Downloads Assume I am in a terminal, and run the command pwd . The output I get is /Users/john/Downloads/ . There are two things I want to accomplish. First, I want to get to my \"lab1\" and run the program. Secondly, I want to create the Visual Basic console app project for my \"lab2\". If we look at the directory tree, we can see that both lab1 and lab2 are directories contained within the directory \"CSE122\". \"CSE122\" is contained in my \"Desktop\" directory, which is under my name. Currently, I am in /Users/john/Downloads/. \"Desktop\" is directly below \"john\", so I first want to get to \"/Users/john/\". To do so, I can use cd .. to move up one directory. Running this command will change my location from /Users/john/Downloads/ to /Users/john/ . Now, I want to get to my \"Desktop\" folder. Since it is directly below \"john\" in the directory tree, I can use cd Desktop to get there. If I run pwd , my current directory is now /Users/john/Desktop/ . However, lab1 and lab2 are in \"CSE122\", not Desktop. \"CSE122\" is directly below Desktop, so to get there I can use cd CSE122 . I am now in /Users/john/Desktop/CSE122 . The first thing I wanted to do was to run my lab1 project. To do so, I should be in the \"lab1\" directory. Once again, I will use cd to get there. cd lab1 will put me in /Users/john/Desktop/CSE122/lab1/ . From here, I can run the command dotnet run to compile and execute my lab1 project. Next, I wanted to create a new project for lab2. The \"lab2\" directory is directly below \"CSE122\", and is not contained within \"lab1\" (use the indention to determine the level of a directory/file). Therefore, to get to \"lab2\", I must first use cd .. to exit the \"lab1\" directory and return to /Users/john/Desktop/CSE122 . Now, I can use cd lab2 to enter the directory /Users/john/Desktop/CSE122/lab2 . To create a new Visual Basic console app project, type the command dotnet new console --language VB . This will create the starter files required for a basic Visual Basic console app. If you run the ls command in the \"lab2\" directory, you will see that it has created a new directory called obj , a file called Program.vb , and a file called lab2.vbproj . Below is an example of the terminal I/O for this example. Lines that start with > are user input lines. Lines that start with # are comments, and are not part of the actual terminal I/O. Lines that do not start with any special characters are the terminal output. # checking my current directory > pwd /Users/john/Downloads # changing directory to my /User/john home directory > cd .. # displaying file listing > ls Desktop Documents Downloads # changing to Desktop and checking contents > cd Desktop > ls CSE122 NET120 # changing to CSE122 and checking contents > cd CSE122 > ls lab1 lab2 # changing to lab1 and running project > cd lab1 > dotnet run Hello World! # moving back a directory so I can change to lab2 > cd .. > cd lab2 # creating a new VB console app in lab2 > dotnet new console --language vb # there is a lot of text output when this happens that is not included here > ls lab2.vbproj obj Program.vb > dotnet run Hello World! >","title":"Understanding the Command Line Environment"},{"location":"NET264/week1/","text":"NET264 - Unix/Linux System Administration: Week 1 Introduction Welcome to NET264 - Unix/Linux System Administration. The prerequisite to this course is NET220 - Unix/Linux Operating Environments, so you are expected to have some familiarity with Linux systems at this point. In this first week, we'll focus on the history of UNIX/Linux, discussing the duties for a Linux Network Administrator, and some information about man pages and getting help with Linux. Finally, we'll cover some installation options you can use to a Linux environment for this class. History of System Administrators For those of you reading the online 4th edition of the book, the section \"A Brief History of System Administration\" can be found after the Index, in the top level of the book's table of contents. The IBM 701 computer was completed in 1952, and was the first commercial computer system. It was quickly replaced 2 years later by the 704, which was totally incompatible with instructions written for the 701, as it used different word-sizes and added floating point arithmetic. The early operators of these laid the foundation for future SysAdmins. The informal meetings between these operators would lead to the IBM funded SHARE group, where operators would share experiences and software. SHARE continues to exist today (https://www.share.org/). These early computer systems were large and expensive, and only really designed to support a single user and task. Therefore, companies would only invest in one if they had a task that was so large and important that it was worth the expense of purchasing and maintaining it. At this point in time, the people in charge of the computers were viewed more as operators than administrators. This began to change in the 1960s, when research into designing multi-user computer systems began. One of the first attempts to develop such an operating system, Multics, was abandoned by the major research group AT&T Bell Labs after 5 years of failed development. Some remnants of this group, Ken Thompson, Rudd Canaday, and Dennis Ritchie, began working on a new system inspried by Multrics, but redesigned it to eliminate many of the pitfalls experienced when developing Multics. This system became known as UNIX, and was originally a small, single-user system. By 1971, this system began implementing many of the commands we still see today in modern Linux, such as as , cat , chdir (now cd ), chmod , chown , cmp , cp , date , and du . By 1973, the advant of the C programming language and idea of piping led to the increase in power of the UNIX operating system, along with a usage and design philosophy: - write programs that do one thing well - write programs that work together - write programs that handle text streams as a universal interface Over the next 15 or so years, the use of UNIX rapidly expanded. Antitrust rulings prevented AT&T from selling UNIX, forcing it to license it to other entities instead. One of the most important recipients of UNIX was Professor Robert Fabry at Cal Berkeley, leading the development of Berkeley UNIX, later to be named the Berkeley Software Distribution (BSD), one of the OS's most famous distribution. Eventually, this development received funding from the US Defense Advanced Research Project Agency (DARPA). However, when AT&T was broken up do to later antitrust rulings, it was then able to develop its own commercial distributions of UNIX and sell them. Eventually, this led to them suing Berkeley Software Design Inc. (BSDI) for stealing it's code. After 2 years, they were able to get a grand total of 3 out of over 18,000 files removed from BSD. However, this uncertainty led to many entities using UNIX to jump ship to Microsoft NT and Windows to avoid being at the mercy of AT&T. Meanwhile, in 1987, Professor Andrew S. Tanenbaum had released a simplified UNIX distribution called MINIX (mini-UNIX) with his book Operating Systems: Design and Implementation . This book and OS led to Linux Torvalds' development of the Linux kernel. The effect of this is that many UNIX SysAdmins shifted to learning Windows in the mid-1990s, to avoid losing their jobs due to their old skills no longer being relevant. As the dust cleared from the late 90s, it was clear that Windows was going to dominate the computing landscape. However, Linux continued to persist, and it became apparent that businesses would have to use both operating systems. Linux servers have much lower total cost of ownership (TCO) than Windows. Sys Admin Roles and Responsibilities The rest of the material in this week is from Chapter 1 of the textbook. If you are using the eBook on Safari, Chapter 1 is in Section One: Basic Administration. While system administrators are not software developers, one of their major roles is scripting, which will be covered in the next two weeks. Here, we'll briefly discuss the other roles of SysAdmins: - Provisioning Accounts: This includes adding new accounts for new users, deleting/deactivating accounts for people who no longer need them, and handling other account issues. - Performing Backups: A boring but critical part of being a SysAdmin. There is nothing exciting about backing up server drives, but are critical to ensuring business operations can continue if a server encounters drive damage. - Installing/Updating Software: New software must be tested before being distributed. Many companies disallow general employees from installing their own software, therefore, SysAdmins must also manage the distribution of new software. Additionally, SysAdmins must manage the updating of software, ensuring that new versions do not cause issues, or quickly distributing new updates that fix a security vulnerability. - System Monitoring: SysAdmins must keep track of the system at all times. Users generally will not notice or report minor issues. There are many software solutions for system monitoring that can aid in this task. - Troubleshooting: Admins must handle the process of diagnosing and fixing issues with the system. - Maintaining Local Documentation: Software and systems are configured to meet business needs, meaning additional documentation beyond what is provided by the software/system authors is required. - Monitoring Security: The SysAdmin is responsible for implementing the company security policy, and ensuring it is followed. They must also update the policy when necessary to reflect changes, as software systems are constantly evolving. SysAdmins must also perform security audits to ensure that systems are not compromised. - Fire Fighting: Hopefully not literally... SysAdmins are responsible for assisting company employees with their computer issues, as well as handling major server failures. Manual Pages These are often abbreviated as \"man pages\", as the command man is used to view them. These are online documentation included with the system or software distribution, and provide information about the command/driver/file format/etc. These do not provide answers to general questions about how the software is working, but provide basic information on what the software does, and how to use it. The man command followed by the name of the command/file/etc. that you want information on is used to lookup the manual for that resource. The PAGER environment variable contains the program that is used to display the man page ( more , less , etc.). The section of the manual you want to view can be passed before the title of the resource to view the manual of. Most commands provide a one-line description. The -k followed by a keyword to search for will return all commands that contain the specified keyword in this description. For example: man -k copy asn1_copy_node (3) - API function bcopy (3) - copy byte sequence copy_file_range (2) - Copy a range of data from one file to another copysign (3) - copy sign of a number copysignf (3) - copy sign of a number copysignl (3) - copy sign of a number cp (1) - copy files and directories # ... many more results truncated Man pages are typically stored in directories under /usr/share/man/ . These pages are typically compressed using the gzip compression format, and decompressed when requested. The default search path(s) is(are) stored in the environment variable MANPATH , and can be checked using the manpath command. Beyond the included documentation installed with the system and software, much of the information can be found on the internet. Nowadays, pretty much all vendors provide documentation on their websites. Additionally, community forums exist where users and SysAdmins share problems and solutions. Requests for Comments (RFC) provide definitive information on internet protocols and procedures. While they are very technical, anyone wanting to know the exact information about a protocol should read the associated RFC. Distros A distribution (often abbreviated to \"distro\") is the Linux kernel (the core operating system) bundled with additional software. All distros aim to provide simple installation, a package manager, and, in the case of desktop distros, some form of GUI and desktop. Many server installations do not ship with a GUI, and instead only run in terminal mode. When looking at distros for business use, it's important to consider the lifetime of the distribution, the vendor support, security update schedule, and if common software is compatible/frequently updated. There is a Linux family tree, of sorts, where there are few core distros that branch from the original kernel. For example, Debian GNU/Linux is one of the older Linux distributions. From Debian, distros including Ubuntu, Kali, and Knoppix are built off of it. Ubuntu is often called a \"cleaned-up\" version of Debian, and offers a more streamlined user experience. Furthermore, Linux Mint and Pop!_OS are built off of Ubuntu (and, by extension, also built off of Debian). Mint is often recommended as a first Linux distro to Windows users due to its simplicity and similarity to Windows' GUI. To provide a breadth of Linux distros, the book will sometimes discuss specifics of Ubuntu (Debian-based), openSUSE (SUSE based), Red Hat (RHEL-based). To provide details of operating systems more directly built off of UNIX, Solaris, HP-UX, and AIX will be discussed. Installation Options As you might imagine, you will need your own Linux environment for this class. There are several options to do so, which we'll discuss here: 1. Virtual Machine: This is the easiest and least intrusive option. One thing to note is that you may need to go into your computer's BIOS/UEFI settings and enable virtualization on your CPU. Then, you can install a hypervisor. If you have a professional or above version of Windows, you can enable Hyper-V by going to \"Turn Windows features on or off\". You can also install VMWare Workstation on Windows or VMWare Fusion on Mac. However, I have heard that, while free, getting Broadcom (the new owner of VMWare) to provide the installer can be challenging. An option that works across operating systems is Oracle VirtualBox. This is a free and open source hypervisor that works on Windows, Mac, and Linux. Your \"host OS\" is the operating system installed on your computer, so make sure to choose the installer for Windows if you are on Windows, and Mac if you are on Mac. To install Linux, you will need to download the .iso file from the vendor (in this case, Ubuntu). For this class, you should use a desktop version. To create a new VM, select the correct option to create a new VM for your hypervisor. The steps will be slightly different, but you will need to tell it how many CPU cores and how much RAM to dedicate to the VM, and select a location to create the Virtual Hard Disk. You can do this on either your main computer's storage drive, or on an external hard drive you USB thumbdrive. Using a thumbdrive or external drive will be slower, but is an option if you do not have very much storage space remaining on your computer. 2. Separate Drive: You can partition and install Linux on a separate hard drive, either by installing an additional hard disk or SSD into your computer if it allows it, or an external hard drive. If it is an external drive, you'll need to make it bootable. A SATA connected drive will be detected as bootable as soon as you install the OS. To install Linux, you will need to download the .iso file from the vendor, in this case, Ubuntu desktop. Next, you will need a USB flash drive large enough (8GB for newer Ubuntu versions, some older ones may work with 4GB) to store the Linux ISO. Rufus is a commonly used tool for this purpose. Here are the instructions from Ubuntu (https://documentation.ubuntu.com/desktop/en/latest/how-to/create-a-bootable-usb-stick/#on-windows) to create a bootable USB on Windows. There are instructions for Mac below the Windows Section. 3. Dual-Boot on Same Drive : This is the most advanced option. You'll need to go into Windows drive management and shrink your Windows partition. 20GB is the minimum recommended amount of drive space for installing Ubuntu. Once you shrink your Windows partition, you'll need to create a bootable CD or USB (same process as option 2) to install Linux. You then need to restart your computer, enter BIOS/UEFI, and boot from the USB. Then, when using the Linux installer, you should see the unallocated space created by shrinking your Windows partition. Use all this space for Linux. Note: A VM is by far the easiest and cheapest option. I would only recommend the other options if you would like to use Linux as an alternate OS in your day-to-day life. Anything for this class, or even any testing you do in the future, can be done using a VM. Installing Linux Once you get to this point, the steps to install Linux will be the same. You will need to configure the partition table for your Linux system. The first 512 bytes should be dedicated for EFI Boot. The next section is the root (signified by a single forward slash / ). Use \"ext4\" as the file system. I'd recommend dedicating at least 4 GB of RAM to it, but 8 would be better, if you have enough storage. If your VM or computer (depending on the installation option) has more than 8 GB of RAM, you can dedicate the rest of the free space to the /home directory. This should also use the ext4 file system. If you have less than 8GB of RAM, I'd recommend dedicating all but 4GB of the remaining free space to /home . After home, add an additional \"swap\" partition. This will provide the OS with virtual RAM to use if it runs out of main memory. From here, you can pretty much just click through all the boxes and the installation will proceed. If you are not using a VM, you may want to consider going back into BIOS/UEFI settings and making your Linux drive/partition the first option. This way, the GRUB menu will appear when you start your system, and you can choose whether to boot Linux, or your original OS. Otherwise, you will have to go into BIOS/UEFI each time you start the system to tell it to boot Linux.","title":"Week 1"},{"location":"NET264/week1/#net264-unixlinux-system-administration-week-1","text":"","title":"NET264 - Unix/Linux System Administration: Week 1"},{"location":"NET264/week1/#introduction","text":"Welcome to NET264 - Unix/Linux System Administration. The prerequisite to this course is NET220 - Unix/Linux Operating Environments, so you are expected to have some familiarity with Linux systems at this point. In this first week, we'll focus on the history of UNIX/Linux, discussing the duties for a Linux Network Administrator, and some information about man pages and getting help with Linux. Finally, we'll cover some installation options you can use to a Linux environment for this class.","title":"Introduction"},{"location":"NET264/week1/#history-of-system-administrators","text":"For those of you reading the online 4th edition of the book, the section \"A Brief History of System Administration\" can be found after the Index, in the top level of the book's table of contents. The IBM 701 computer was completed in 1952, and was the first commercial computer system. It was quickly replaced 2 years later by the 704, which was totally incompatible with instructions written for the 701, as it used different word-sizes and added floating point arithmetic. The early operators of these laid the foundation for future SysAdmins. The informal meetings between these operators would lead to the IBM funded SHARE group, where operators would share experiences and software. SHARE continues to exist today (https://www.share.org/). These early computer systems were large and expensive, and only really designed to support a single user and task. Therefore, companies would only invest in one if they had a task that was so large and important that it was worth the expense of purchasing and maintaining it. At this point in time, the people in charge of the computers were viewed more as operators than administrators. This began to change in the 1960s, when research into designing multi-user computer systems began. One of the first attempts to develop such an operating system, Multics, was abandoned by the major research group AT&T Bell Labs after 5 years of failed development. Some remnants of this group, Ken Thompson, Rudd Canaday, and Dennis Ritchie, began working on a new system inspried by Multrics, but redesigned it to eliminate many of the pitfalls experienced when developing Multics. This system became known as UNIX, and was originally a small, single-user system. By 1971, this system began implementing many of the commands we still see today in modern Linux, such as as , cat , chdir (now cd ), chmod , chown , cmp , cp , date , and du . By 1973, the advant of the C programming language and idea of piping led to the increase in power of the UNIX operating system, along with a usage and design philosophy: - write programs that do one thing well - write programs that work together - write programs that handle text streams as a universal interface Over the next 15 or so years, the use of UNIX rapidly expanded. Antitrust rulings prevented AT&T from selling UNIX, forcing it to license it to other entities instead. One of the most important recipients of UNIX was Professor Robert Fabry at Cal Berkeley, leading the development of Berkeley UNIX, later to be named the Berkeley Software Distribution (BSD), one of the OS's most famous distribution. Eventually, this development received funding from the US Defense Advanced Research Project Agency (DARPA). However, when AT&T was broken up do to later antitrust rulings, it was then able to develop its own commercial distributions of UNIX and sell them. Eventually, this led to them suing Berkeley Software Design Inc. (BSDI) for stealing it's code. After 2 years, they were able to get a grand total of 3 out of over 18,000 files removed from BSD. However, this uncertainty led to many entities using UNIX to jump ship to Microsoft NT and Windows to avoid being at the mercy of AT&T. Meanwhile, in 1987, Professor Andrew S. Tanenbaum had released a simplified UNIX distribution called MINIX (mini-UNIX) with his book Operating Systems: Design and Implementation . This book and OS led to Linux Torvalds' development of the Linux kernel. The effect of this is that many UNIX SysAdmins shifted to learning Windows in the mid-1990s, to avoid losing their jobs due to their old skills no longer being relevant. As the dust cleared from the late 90s, it was clear that Windows was going to dominate the computing landscape. However, Linux continued to persist, and it became apparent that businesses would have to use both operating systems. Linux servers have much lower total cost of ownership (TCO) than Windows.","title":"History of System Administrators"},{"location":"NET264/week1/#sys-admin-roles-and-responsibilities","text":"The rest of the material in this week is from Chapter 1 of the textbook. If you are using the eBook on Safari, Chapter 1 is in Section One: Basic Administration. While system administrators are not software developers, one of their major roles is scripting, which will be covered in the next two weeks. Here, we'll briefly discuss the other roles of SysAdmins: - Provisioning Accounts: This includes adding new accounts for new users, deleting/deactivating accounts for people who no longer need them, and handling other account issues. - Performing Backups: A boring but critical part of being a SysAdmin. There is nothing exciting about backing up server drives, but are critical to ensuring business operations can continue if a server encounters drive damage. - Installing/Updating Software: New software must be tested before being distributed. Many companies disallow general employees from installing their own software, therefore, SysAdmins must also manage the distribution of new software. Additionally, SysAdmins must manage the updating of software, ensuring that new versions do not cause issues, or quickly distributing new updates that fix a security vulnerability. - System Monitoring: SysAdmins must keep track of the system at all times. Users generally will not notice or report minor issues. There are many software solutions for system monitoring that can aid in this task. - Troubleshooting: Admins must handle the process of diagnosing and fixing issues with the system. - Maintaining Local Documentation: Software and systems are configured to meet business needs, meaning additional documentation beyond what is provided by the software/system authors is required. - Monitoring Security: The SysAdmin is responsible for implementing the company security policy, and ensuring it is followed. They must also update the policy when necessary to reflect changes, as software systems are constantly evolving. SysAdmins must also perform security audits to ensure that systems are not compromised. - Fire Fighting: Hopefully not literally... SysAdmins are responsible for assisting company employees with their computer issues, as well as handling major server failures.","title":"Sys Admin Roles and Responsibilities"},{"location":"NET264/week1/#manual-pages","text":"These are often abbreviated as \"man pages\", as the command man is used to view them. These are online documentation included with the system or software distribution, and provide information about the command/driver/file format/etc. These do not provide answers to general questions about how the software is working, but provide basic information on what the software does, and how to use it. The man command followed by the name of the command/file/etc. that you want information on is used to lookup the manual for that resource. The PAGER environment variable contains the program that is used to display the man page ( more , less , etc.). The section of the manual you want to view can be passed before the title of the resource to view the manual of. Most commands provide a one-line description. The -k followed by a keyword to search for will return all commands that contain the specified keyword in this description. For example: man -k copy asn1_copy_node (3) - API function bcopy (3) - copy byte sequence copy_file_range (2) - Copy a range of data from one file to another copysign (3) - copy sign of a number copysignf (3) - copy sign of a number copysignl (3) - copy sign of a number cp (1) - copy files and directories # ... many more results truncated Man pages are typically stored in directories under /usr/share/man/ . These pages are typically compressed using the gzip compression format, and decompressed when requested. The default search path(s) is(are) stored in the environment variable MANPATH , and can be checked using the manpath command. Beyond the included documentation installed with the system and software, much of the information can be found on the internet. Nowadays, pretty much all vendors provide documentation on their websites. Additionally, community forums exist where users and SysAdmins share problems and solutions. Requests for Comments (RFC) provide definitive information on internet protocols and procedures. While they are very technical, anyone wanting to know the exact information about a protocol should read the associated RFC.","title":"Manual Pages"},{"location":"NET264/week1/#distros","text":"A distribution (often abbreviated to \"distro\") is the Linux kernel (the core operating system) bundled with additional software. All distros aim to provide simple installation, a package manager, and, in the case of desktop distros, some form of GUI and desktop. Many server installations do not ship with a GUI, and instead only run in terminal mode. When looking at distros for business use, it's important to consider the lifetime of the distribution, the vendor support, security update schedule, and if common software is compatible/frequently updated. There is a Linux family tree, of sorts, where there are few core distros that branch from the original kernel. For example, Debian GNU/Linux is one of the older Linux distributions. From Debian, distros including Ubuntu, Kali, and Knoppix are built off of it. Ubuntu is often called a \"cleaned-up\" version of Debian, and offers a more streamlined user experience. Furthermore, Linux Mint and Pop!_OS are built off of Ubuntu (and, by extension, also built off of Debian). Mint is often recommended as a first Linux distro to Windows users due to its simplicity and similarity to Windows' GUI. To provide a breadth of Linux distros, the book will sometimes discuss specifics of Ubuntu (Debian-based), openSUSE (SUSE based), Red Hat (RHEL-based). To provide details of operating systems more directly built off of UNIX, Solaris, HP-UX, and AIX will be discussed.","title":"Distros"},{"location":"NET264/week1/#installation-options","text":"As you might imagine, you will need your own Linux environment for this class. There are several options to do so, which we'll discuss here: 1. Virtual Machine: This is the easiest and least intrusive option. One thing to note is that you may need to go into your computer's BIOS/UEFI settings and enable virtualization on your CPU. Then, you can install a hypervisor. If you have a professional or above version of Windows, you can enable Hyper-V by going to \"Turn Windows features on or off\". You can also install VMWare Workstation on Windows or VMWare Fusion on Mac. However, I have heard that, while free, getting Broadcom (the new owner of VMWare) to provide the installer can be challenging. An option that works across operating systems is Oracle VirtualBox. This is a free and open source hypervisor that works on Windows, Mac, and Linux. Your \"host OS\" is the operating system installed on your computer, so make sure to choose the installer for Windows if you are on Windows, and Mac if you are on Mac. To install Linux, you will need to download the .iso file from the vendor (in this case, Ubuntu). For this class, you should use a desktop version. To create a new VM, select the correct option to create a new VM for your hypervisor. The steps will be slightly different, but you will need to tell it how many CPU cores and how much RAM to dedicate to the VM, and select a location to create the Virtual Hard Disk. You can do this on either your main computer's storage drive, or on an external hard drive you USB thumbdrive. Using a thumbdrive or external drive will be slower, but is an option if you do not have very much storage space remaining on your computer. 2. Separate Drive: You can partition and install Linux on a separate hard drive, either by installing an additional hard disk or SSD into your computer if it allows it, or an external hard drive. If it is an external drive, you'll need to make it bootable. A SATA connected drive will be detected as bootable as soon as you install the OS. To install Linux, you will need to download the .iso file from the vendor, in this case, Ubuntu desktop. Next, you will need a USB flash drive large enough (8GB for newer Ubuntu versions, some older ones may work with 4GB) to store the Linux ISO. Rufus is a commonly used tool for this purpose. Here are the instructions from Ubuntu (https://documentation.ubuntu.com/desktop/en/latest/how-to/create-a-bootable-usb-stick/#on-windows) to create a bootable USB on Windows. There are instructions for Mac below the Windows Section. 3. Dual-Boot on Same Drive : This is the most advanced option. You'll need to go into Windows drive management and shrink your Windows partition. 20GB is the minimum recommended amount of drive space for installing Ubuntu. Once you shrink your Windows partition, you'll need to create a bootable CD or USB (same process as option 2) to install Linux. You then need to restart your computer, enter BIOS/UEFI, and boot from the USB. Then, when using the Linux installer, you should see the unallocated space created by shrinking your Windows partition. Use all this space for Linux. Note: A VM is by far the easiest and cheapest option. I would only recommend the other options if you would like to use Linux as an alternate OS in your day-to-day life. Anything for this class, or even any testing you do in the future, can be done using a VM.","title":"Installation Options"},{"location":"NET264/week1/#installing-linux","text":"Once you get to this point, the steps to install Linux will be the same. You will need to configure the partition table for your Linux system. The first 512 bytes should be dedicated for EFI Boot. The next section is the root (signified by a single forward slash / ). Use \"ext4\" as the file system. I'd recommend dedicating at least 4 GB of RAM to it, but 8 would be better, if you have enough storage. If your VM or computer (depending on the installation option) has more than 8 GB of RAM, you can dedicate the rest of the free space to the /home directory. This should also use the ext4 file system. If you have less than 8GB of RAM, I'd recommend dedicating all but 4GB of the remaining free space to /home . After home, add an additional \"swap\" partition. This will provide the OS with virtual RAM to use if it runs out of main memory. From here, you can pretty much just click through all the boxes and the installation will proceed. If you are not using a VM, you may want to consider going back into BIOS/UEFI settings and making your Linux drive/partition the first option. This way, the GRUB menu will appear when you start your system, and you can choose whether to boot Linux, or your original OS. Otherwise, you will have to go into BIOS/UEFI each time you start the system to tell it to boot Linux.","title":"Installing Linux"},{"location":"NET264/week2/","text":"NET264 - Unix/Linux System Administration: Weeks 2 and 3 These two weeks will cover shell scripting, Python scripting, regular expressions, and the Vim editor as they pertain to being a Linux system admin. These weeks correspond to Chapter 7 in the 5th edition of the book, and Chapter 2 of the 4th edition book. For this class, you only need to worry about Python, and not Perl or Ruby which are mentioned in the book. Introduction Scripting allows you to automate tasks that are boring and repetitive. This reduces the chance for error, and provides documentation for how these tasks are completed. The focus in this class is on smaller, day to day scripts you'd write as a sysadmin, rather than larger scale scripting for fully automated server deployment, for example. We'll cover both shell and Python scripting. Shell scripts are more portable, as they are typically very small and only invoke commands that are available on pretty much any Unix-like OS. Python scripting is much more powerful, but much of Python's power comes from its rich ecosystem of 3rd party libraries and modules, making it less portable across different systems without significant setup. Shell Basics This should mostly be a review of things you learned in NET220. But, if it's been a while, this can be a helpful refresher. Redirection and Piping The most powerful component of command shells and shell scripting is redirection and piping. This allows commands to interact with each other's output. The shell has three communication channels it makes available to processes: STDOUT (standard output), STDIN (standard input), and STDERR (standard error output). Each open I/O channel is provided a file descriptor, a small integer than provides a way to reference open files. STDIN is always 0, STDOUT is always 1, and STDERR is always 2. In a typical interactive shell session, STDIN is read from the keyboard, and STDOUT and STDERR are written to the terminal on the computer monitor. The redirection operators (<, >, >>) allow you to change the destination of the STD streams. The < symbol allows a command's STDIN to be connected to a file. The > symbol redirects STDOUT from a command to a file. > will overwrite the original contents of the file. If you wish to append STDOUT to a file, you should use >> instead. To redirect STDOUT and STDERR to the same place, use &> or &>>. To redirect only STDERR, use 2> or 2>>. Redirecting STDERR can be useful when a command may produce many error messages that are not important. For example, if you are running the command find without sudo , you will get many \"Permission denied\" errors when it tries to search directories you do not have access to. To ignore these, you can use the file /dev/null to dump the output of STDERR: # searching for pdf files find -name '*.pdf' 2> /dev/null The above command will only report files that are located to the terminal. All the \"Permission denied\" errors are sent away to /dev/null. If you have many results, you may wish to store them into a file to be searched later. You can dump STDOUT and STDERR to separate locations: find -name '*.pdf' > PDFs.txt 2> /dev/null This will write all the PDF files located by find to a file called PDFs.txt, while the error messages are still discarded. What if you want to chain two commands together? The pipe operator, | , can be used for that. | will redirect the STDOUT of one command to the STDIN of the next. For example, I may want to find PDFs in my current directory. I can pipe the output of ls into grep : ls -la | grep -i .pdf This will display all files that contain '.pdf' in their name. Similar to these concepts, you can conditionally run commands based on the success/failure of the preceding command. The && operator can be used to only run a second command if the first succeeds. For example, cp /usr/share/shared.txt . && cat shared.txt will only run cat shared.txt if shared.txt was successfully copied from /usr/share . To run a command if the preceding command fails, use the || operator. You can use a \\ at the end of a line to continue the same command on the next line. To have multiple commands on the same line, separate them using ; . Variables Shell variables allow you to provide a name to access a value. For example, if you have a long directory name that you will be using frequently, you can store its value in a variable to make it easier to access. To create a variable, type the name of the variable, followed immediately by an equal sign and the value of the variable. You cannot have any spaces between the name, equal sign, and value. The value can be wrapped in quotes. If the value has any spaces in it, it must be wrapped in quotes. Below is an example: long_dir='/home/me/Documents/NET264/Week2/Materials/week2_notes.txt` To access the variable's value, prefix it with a dollar sign. Otherwise, the shell will interpret the name as a text literal and not a variable: echo long_dir # will print: long_dir echo $long_dir # will print: /home/me/Documents/NET264/Week2/Materials/week2_notes.txt You may wrap variable name's in curly braces {} to help distinguish them. While not always required, they can be helpful when the variable's value is part of a text string. For example: josh='Josh' # this will print \"Josh is short for Joshua\" # we need the curly braces to separate the # variable from the \"ua\" part echo \"$josh is short for ${josh}ua\" # we could also put it on the first $josh echo \"${josh} is short for ${josh}ua\" There aren't formal standards for naming shell variables, but all caps names typically suggest that an environment variable or configuration variable. For general scripting, stick to snake_case names, which are all lowercase, and separate words with underscores. Quotes Different styles of quotes mean different things in the shell. Strings with double quotes will replace embedded variables with their values: name=\"World\" echo \"Hello ${name}!\" # prints: Hello World! Single quotes treat all of their contents as literal text: name=\"World\" echo \"Hello ${name}!\" # prints: Hello ${name}! Backticks allow you to embed shell commands within a double-quoted string: # pwd will be replaced with the current working directory, # and ls will be replaced with the files in that directory echo \"Directory listing for `pwd`: `ls`\" Filtering Commands cut : prints sections of its input lines sort : sort lines uniq : print unique lines (and their counts) wc : word count (along with lines and characters). Use the -l , -w , or -c to specify which to print tee : copy input to two places. Good if you need to send the same output to your terminal and a file head : reads the first n lines of a file tail : read the last n lines of a file grep : searches text for regular expression and returns results. We will discuss this more in-depth later Shell Scripting These instructions are for the GNU Bourne-Again SHell (bash), but will apply to most other command shells like dash, zsh, etc. It will not apply to more programming language-like shells such as csh and tcsh. bash is good for smaller scripts (50 to 100 lines max). Writing bash scripts is basically like taking the commands you'd write in the shell, and putting them in a file to be run all at once. The main way to develop bash scripts is to test the commands you need to run in the terminal. Once you are satisfied, you can write them out in your script. Like in the terminal, comments in bash start with the pound symbol # . You can redirect and pipe the output of commands just as you would in the normal terminal shell. Below is a simple bash script to write \"Hello World!\" to the console. helloworld #!/bin/bash echo \"Hello World!\" The first line is a \"shebang\" comment, and indicates to the OS kernel that this file is meant to be run by the program /bin/bash . We could alternatively put /bin/zsh if we wanted to use zsh to run this script. We can even write scripts for Python by putting #!/bin/python3 , although bash scripts are not valid Python and the interpreter would crash. There multiple options to run this script. As a plain text file, we can call the shell interpreter we want to use and pass it the script as a command line argument: bash helloworld This will create a new instance of bash to run the command that will immediately end when the script has completed. Using the source command will run the script in your current shell session: source helloworld Neither of these options require the script file to be executable, and will ignore the shebang comment at the top. If we want to run the script directly, like we would a program, we must first make it executable using the chmod command: chmod +x helloworld ./helloworld In this case, the OS will read the shebang comment at the top of the file, and use that command interpreter to execute the script. If there is no shebang comment, the OS will use the shell that the script was run from to execute it. You may notice that the helloworld script has no file extension. Unix-like operating systems do not use extensions to identify file types like Windows does. With that being said, shell scripts often have the extension .sh to indicate they are scripts, and to differentiate them from directories and executable binaries. Building Scripts Often, we would like to test our commands before hiding them away in a script to be run indiscriminately in the future. We build up scripts by running our desired commands in the terminal, then, when satisfied, copy the commands into our script file. To avoid making potentially unwanted changes when testing the script, you can use echo statements to print the commands you plan on running, rather than executing them. This way, you can check the value of variables and how they are formatted within the commands before running the command. Input/Output echo and printf are the main commands for writing output text. printf is more powerful, but also more finicky to use in some cases. Input can be read from STDIN using the read command. The script below will ask the user for their name and print it back out to them. echo -n \"What is your name?\" read name echo \"Your name is ${name}.\" exit 0 This is okay, but what if the user types nothing and hits enter? We can check that the input is not blank using an if statement. We will cover if more in-depth later, but for now it should be obvious what its purpose is: echo -n \"What is your name?\" read name # -n \"${name}\" is true if the value of name is not null if [ -n \"${name}\" ] ; then echo \"Your name is ${name}.\" exit 0 else # >&2 redirects output to STDERR, since it is an error message echo \"You forgetting something...?\" >&2 exit 1 fi Command-Line Arguments Typically, scripts do not get interactive user input. Often, information is passed into scripts via \"command-line arguments\". These are values that you provide to the script when you run it. Let's consider this script below that copies the contents of a source directory to a target. It is using interactive user input to get the paths of the two directories: copy_directory_contents.sh echo -n \"Enter source directory: \" read src_dir # the -d checks if the value of src_dir is a directory # the ! out in front negates it, so our check is # if src_dir is not a directory if ! [ -d $src_dir ] ; then echo \"${src_dir} is not a directory...\" >&2 exit 1 fi echo -n \"Enter target directory: \" read dst_dir # same check for dst_dir if ! [ -d $dst_dir ] ; then echo \"${dst_dir} is not a directory...\" >&2 exit 2 fi cp -r src_dir/* dst_dir/ exit 0 Let's rewrite this script so that we use command-line arguments to specify the directories. Command-line arguments are named $0 , $1 , $2 , etc. for each argument. $0 is always the command used to run the script. $1 is the first command-line argument, $2 is the second, etc. The variable $# is the number of command line arguments, not counting $0 . So, if we ran ./copy_directory_contents.sh /my/source/directory /my/target/directory , the value of $0 would be ./copy_directory_contents.sh , $1 would be /my/source/directory , and $2 would be /my/target/directory . The value of $# would be 2. With this in mind, let's write the modified script that uses command-line arguments: copy_directory_contents.sh # -ne is not equal # this first check verifies if there are 2 command line args if [ 2 -ne $# ] ; then echo \"Must provide source and target directory\" >&2 echo \"Usage: $0 [source_directory] [target_directory]\" >&2 exit 1 fi src_dir=$1 if ! [ -d $src_dir ] ; then echo \"${src_dir} is not a directory...\" >&2 exit 2 fi dst_dir=$2 if ! [ -d $dst_dir ] ; then echo \"${dst_dir} is not a directory...\" >&2 exit 3 fi cp -r src_dir/* dst_dir/ exit 0 The script first ensures that there are two command-line arguments. The -ne checks for not equal, so if $# is not equal to 2, an error message is printed and the script aborts. The bash variable src_dir is given the value of the first command-line argument, $1 , and the variable dst_dir is given the value of the second, $2 . Error messages are printed if either is not a directory. Functions Perhaps we'd also like to print the usage message each time there is an error. We could copy the echo statement that contains the script's usage into each if statement, but that means that we have to remember to update the usage message in all three places if we change how the script should be used in the future. When there is a sequential set of commands that you want to run in multiple places, you can write a function to do so. A function is like a user-defined command. Anywhere you put the name of the function (often referred to as \"calling the function\" or \"invoking the function\"), you can think of it as the commands within the function being copy-pasted to that spot. function show_usage { echo \"Usage: $0 [source_directory] [target_directory]\" >&2 } if [ 2 -ne $# ] ; then echo \"Must provide source and target directory\" >&2 show_usage exit 1 fi src_dir=$1 if ! [ -d $src_dir ] ; then echo \"${src_dir} is not a directory...\" >&2 show_usage exit 2 fi dst_dir=$2 if ! [ -d $dst_dir ] ; then echo \"${dst_dir} is not a directory...\" >&2 show_usage exit 3 fi cp -r src_dir/* dst_dir/ exit 0 Just like you can pass arguments to scripts and commands, you can also pass arguments to functions. A function's arguments are accessed exactly like command-line arguments to the script. If you are going to have a function that takes arguments, a comment explaining the arguments and their expected order should be provided. Let's modify the script again to use a function to handle printing all the script's error messages and aborting: # print_error_and_exit [error_message] [exit_code] # error_message - message printed before the usage # exit_code - code to return to OS when exiting script function print_error_and_exit { # checking if custom error message is provided if [ -n $1 ] ; then echo $1 >&2 else # printing default error message if none provided echo \"An error ocurred\" fi echo \"Usage: $0 [source_directory] [target_directory]\" >&2 # checking if error code is provided if [ -n $2 ] ; then exit $2 else # exit with non-zero code exit 99 fi } if [ 2 -ne $# ] ; then print_error_and_exit \"Must provide source and target directory\" 1 fi src_dir=$1 if ! [ -d $src_dir ] ; then print_error_and_exit \"${src_dir} is not a directory...\" 2 fi dst_dir=$2 if ! [ -d $dst_dir ] ; then print_error_and_exit \"${dst_dir} is not a directory...\" 3 fi cp -r src_dir/* dst_dir/ exit 0 Variable Scope All variables in bash are global, meaning their value is visible and changeable in all locations of the script. However, functions can temporarily claim a variable's name inside itself by using local to localize the name. Otherwise, variables can have their values overwritten by the function. These two example scripts will illustrate this: no_local.sh function foo { echo \"x at start of function = ${x}\" x=\"world hello\" echo \"x at end of function = ${x}\" } x=\"hello world\" echo \"x before function = ${x}\" foo echo \"x after function = ${x}\" Output: x before function = hello world x at start of function = hello world x at end of function = world hello x after function = world hello As you can see, after the function was called, x has a different value. If you want to prevent variables within the function from impacting the script outside the function, you should declare them as local : local.sh function foo { echo \"x at start of function = ${x}\" local x=\"world hello\" echo \"x at end of function = ${x}\" } x=\"hello world\" echo \"x before function = ${x}\" foo echo \"x after function = ${x}\" Output: x before function = hello world x at start of function = hello world x at end of function = world hello x after function = hello world Conditional Statements We saw if statements earlier. Now, let's take a look at them more in depth. As the name would suggest, the commands within an if statement only execute if the condition that is being checked is true. Optionally, the if may contain an else that indicates what do if the condition is false. To check multiple conditions, elif can be used after the first if , but before any else . An if statement can only have one if at the beginning, and one else at the end. You can put as many elif s between as you would like. If statements always start with the word if , and end with fi , which comes after all elif and else blocks. The if and elif keywords are followed by conditions , which are tested to be true or false . Conditions are almost always wrapped in square brackets: [ condition ], which is inherited from the original Bourne shell. Some people wrap bash conditions in two sets of square brackets: [[ condition ]]. This is not guaranteed to be portable to shells other than bash, however. Operators Below is a table of comparison operators for bash variables. Symbols ( = , < , > , etc.) are used to compare values as text strings, while text \"operators\" ( -eq , -ne , etc.) are used to compare the values of variables as numbers. String Number English Interpretation x = y x -eq y x is equal to y x != y x -ne y x is not equal to y x < y x -lt y x is less than y x -le y x is less than or equal to y x > y x -gt y x is greater than y x -ge y x is greater than or equal to y -n x x is not null (null means empty) -z x x is null There are also special operators to treat the value being tested as a file path: | Operator | Interpretation | | -e path | path exists | | -d path | path is a directory | | -f path | path is a file | | -s path | path exists and is not empty | | -r path | You have read permission for path | | -w path | You have write permission for path | | path1 -nt path2 | path1 is newer than path2 | | path1 -ot path2 | path1 is older than path2 | A condition can be negated by putting an exclamation point ! before the condition. Now that we have the basics, let's write a basic script using if , elif , and else . This script will take a path as its argument, and determine if it is a file, directory, or empty. #!/bin/bash path=$1 # using -z to check if path is null if [[ -z $path ]] ; then echo \"Need to provide path\" >&2 exit(1) # -e path checks if it exists # putting ! before the square brackets negates it elif ! [[ -e $path ]] ; then echo \"$path does not exist\" # -f checks if the path is to a file elif [[ -f $path ]] ; then echo \"$path is a file\" # at this point, the path exists and is not a file # therefore, it must be a directory else echo \"$path is a directory\" fi exit(0) Loops Loops allow a set of instructions to be repeated. There are two types of loop in bash, for and while . for loops start with the keyword for followed by a control. Afterwards, you need the word do . Typically, a semicolon is used to allow the do to be placed on the same line as for . The loop is ended with the word done . Otherwise, do has to go on its own line. You can create the typical for loop you'd see in programming languages like C: for ((i=0; i<$MAX; i++)); do echo \"$i\" done Often, though, bash for-loops are used to iterate through file globs. Hopefully you remember from your previous Linux class that you can use wildcards like * to create a basic pattern. Something like *.pdf would be a collection of all files that end with \".pdf\". Below is a for loop that finds all files named with \".PDF\" and renames them to \".pdf\". for $f in *.PDF ; do mv \"$f%.PDF\" \"$f.pdf\" done While loops can be used when the number of times the loop must run is indeterminate. The syntax is otherwise the same as a for loop. Below is an example of reading the lines of a file and printing them to the console: # reads the content of the first command line arg # into STDIN exec 0<$1 line_number=1 # read command reads from STDIN until newline is found # since the file is in STDIN, this effectively reads # a line from the file into the variable line while read line; do echo \"$line_number $line\" $((counter++)) done Arithmetic You've seen in a couple places in the previous sections that there have been cases where bash variables have been wrapped in two sets of parenthesis. For example, the $((counter++)) in the previous example. As we've discussed, all bash variables are treated as text strings. So, what if we need to treat them as numbers. That is where the double parenthesis come in. When a bash variable is wrapped in double parenthesis, it is treated as a number instead of a string. This doesn't just apply to variables; raw numbers need to be wrapped in parentheses too if you want them to be treated as numbers. #!/bin/bash # arith.sh x=1 y=$((2)) z=$x+$y a=$((x+y)) # even if using a number, still need (()) b=$((x+1)) echo $z # prints: 1+2 echo $a # prints: 3 echo $b # prints: 2 Arrays The last bash topic we'll cover is arrays. Arrays are a collection of values stored under a single name. The individual values, often called elements, are accessed via a subscript (also called index). Below is an example of a bash array, and how to access its elements: # arrays are declared like normal variables, # but the elements are wrapped by parentheses # elements are separated by spaces array=(a b c d e f g) array_size=7 # the first index in an array is 0 for ((i=0; i<$array_size; i++)) ; do # individual elements are accessed using # square brackets with the desired index # within them echo \"${array[$i]}\" done # you can also update the values array[0]=A There are several peculiarities with bash arrays, which are detailed in the book. In general, if your script is complicated enough that you need to use an array, you might be better served writing it in Python... Python Released in 1991, Python was created by Guido van Rossum (aka the Benevolent Dictator for Life). The current version, python3 , comes preinstalled on most Linux distributions. A rather ironic line from the 4th edition of the book is \"This section describes Python 2. Python 3 is in the works and is likely to be released during the lifetime of this book. But unlike Perl 6, it appears likely to be a relatively incremental update.\" While it is true that Perl 6 is very different to Perl 5 (so much so that it is now called Raku), Python 3 is perhaps the most famous and extreme example of a programming language totally breaking backwards compatibility with its previous major release. With that said, the world has moved on, and python3 is Python, with Python 2 support ending entirely in 2020. However, the executable is still called python3 on many systems. In fact, that there is a popular package called python-is-python3 that exists solely to create a symlink to allow python3 to be run using the python command. With our trip down memory lane out of the way, let's get into the meat and potatoes of Python. Python is a powerful scripting and general purpose programming language. There is a large community of official and third-party libraries and modules to support programming and scripting in Python. This is a blessing and a curse. This ecosystem allows for you to write powerful scripts quickly, but you may also fall into what is known as \"dependency hell\", where your script needs many software libraries to be installed alongside it to function correctly. Worse yet, you often need the correct versions of these libraries, otherwise they may not play nicely with each other. Luckily, the scripts we write in this class will not be using any third-party libraries. We will stick to only using libraries that are installed alongside Python. For a general tutorial on Python, check out https://www.w3schools.com/python/. Where to Write Python There are several options to write Python. First, you can start an interactive shell session by typing python or python3 into a terminal. This will switch your bash shell to a Python shell. You can write Python statements after the >>> , which is called a prompt: Python from the shell You can exit the shell by typing exit() , quit() , or CTRL + D. If you are using a Linux distribution that includes a desktop GUI, you can run idle from the terminal to launch the Python Integrated Development and Learning Environment. If it is not installed, you will likely be given the instruction to do so. On Ubuntu, you should run sudo apt install idle . idle opens up a window with an interactive Python shell environment. You also have the option create and open Python script files from the \"File\" tab in the top left corner. The IDLE editor provides some limited syntax highlighting and tooltips. The next option is to write your scripts directly in a text file. If you wish to run your Python scripts like a bash script, using the ./script_name syntax, you will need to include a shebang comment at the top: #!/bin/python3 . Python files are often run using the interpreter, like: python3 script_name . If you only plan to run your scripts using python3 , you do not need the shebang. Python scripts typically have the .py extension to indicate that they are Python. Like bash, statements in Python are ended by a line return. Also like bash, if you want to put multiple statements on the same line, you can separate them using a semicolon. Variables Python variables are much easier than bash. There is no more attaching dollar signs to the name, nor the need to have no spaces in the original declaration. A variable in Python can be named using any combination of letters, numbers, and the underscore character, however, it must not start with a number. Most Python programmers use snake_case for their variable names, just as we saw with bash. Variables whose values are not meant to be changed anywhere in the script are written using SCREAMING_SNAKE_CASE . Unlike bash, where all variables are treated as text strings, Python variables have a data type associated with them. Python is dynamically typed, meaning the interpreter infers the type of the variable; you do not need to provide it. Python has four primitive types: int (whole numbers), float (decimal numbers), bool ( True and False ), and str (any text string). Below is an example of some Python variables: # comments in Python start with a #, just like in bash x = 2 # since 2 is a whole number, x is an int y = 2.2 # since 2.2 is a decimal number, y is a float z = True # z is a bool s = \"Some text\" # s has the str (string) type Output and Input Python provides the print function to write output to the terminal. Print automatically adds a new line to the end. # Text in Python must always use quotation marks print(\"Hello World!\") print('Hello World!') x = 1 y = 2 z = 3 # commmas can be used to print multiple values # the values will be separated by spaces print(x, y, z) # prints: 1 2 3 # you can use the \"sep=\" argument to change the separator print(x, y, z, sep=\"\") # prints: 123 # to change the line end, you can use the \"end=\" argument print(\"hello\", end=\", \") print(\"world\", end=\"!\\n\")# together, these print: hello, world! Input is read using the input function. Optionally, you may provide a message to be printed when the input function prompts for input. To get the result of the function, you must set a variable equal to it. Note: the input function always treats input as a string. Even if I enter \"5\", it will not treat it as an int . I must explicitly cast the result as an int . See the example: # a name is a text string, # so we don't need to convert it to anything else name = input(\"Enter your name: \") print(\"Your name is\", name) # wrap input with int() to cast input as int age = int(input(\"Enter your age (in whole years): \")) print(\"Your age is\", age) # the same goes for float gpa = float(input(\"Enter your GPA: \")) print(\"Your GPA is\", gpa) Command-Line Arguments Python scripts can use command-line arguments, just like bash scripts can. However, they are accessed differently. To access the command-line arguments, we will need to introduce our first library/module. The sys module contains functions and objects that relate to Operating System. This includes argv , which is a list containing the command line arguments. A Python list is like a bash array, only much more powerful and useable. Below is some code to read the command-line arguments: # student_info.py # this imports the argv list from the sys module from sys import argv # like bash, the first command-line argument # is the command that ran the script script = argv[0] # assuming we have the same input as the # last example, but using command-line args name = argv[1] age = int(argv[2]) gpa = float(argv[3]) print(\"The script you ran is:\", script) print(\"Your name is\", name) print(\"Your age is\", age) print(\"Your GPA is\", gpa) We can run this script with: python3 student_info.py Adam 21 3.5 The output of the script would be: The script you ran is: student_info.py Your name is Adam Your age is 21 Your GPA is 3.5 Of course, we should also have all the error checking and handling we did for the bash scripts. I've skipped it to keep the examples shorter and less cluttered. Formatted Strings In bash, we are able to inject variables directly into double-quoted strings. This is mainly thanks to the fact that they start with a dollar sign, allowing the bash interpreter easily to find and replace the variables with their actual value. Since Python does not use special characters to mark its variables, we cannot directly put variables into strings. We can get a similar effect by using formatted (also called interpolated) strings, often called f-strings in Python. An f-string has the letter f directly in front of the double-quotes. Inside the string, we can place curly braces anywhere we want to insert a variable. Let's rewrite the previous script using f-strings to show how this works: # student_info.py # this imports the argv list from the sys module from sys import argv # like bash, the first command-line argument # is the command that ran the script script = argv[0] # assuming we have the same input as the # last example, but using command-line args name = argv[1] age = int(argv[2]) gpa = float(argv[3]) print(f\"The script you ran is: {script}\") print(f\"Your name is {name}\") print(f\"Your age is {age}\") print(f\"Your GPA is {gpa}\") Functions Python's functions are much like bash functions in terms of where you'd use them, however, they are declared and implemented differently. Python functions start with def , which is followed by the name of the function, its parameter list (which may be empty), and a colon. Here, we get to one of the peculiarities of Python: significant whitespace. In bash, the start and end of the function was specified using curly braces. Any commands between the opening and closing brace was considered to be part of the function. Python does not use curly braces to delineate blocks. Instead, it uses indentation. See the example below: # def is the keyword that tells python3 this is a function # add_and_print is the name of the function # x and y are the parameters # parameters are the names of the function's arguments # Python does not use $1, $2, etc. to access function arguments # instead, you must provide them a name # parameters are separated by commmas # parameters must be placed within the parenthesis # if a function has no parameters, you must still include empty parenthesis # the parameter list is followed by a colon def add_and_print(x, y): # the statements within the function must be indented z = x + y print(f\"{x} + {y} = {z}\") # returning to the previous level of indentation exits the function body a = 1 b = 2 # the function can be called using variables # the arguments must go in parenthesis add_and_print(a, b) # you can also use literal values add_and_print(3, 4) # or a combination add_and_print(a, 6) The number of arguments provided when calling the function must match the number of parameters. What if we want parameters to be optional? For that, we can provide a default value. Let's rewrite the print_error_and_exit function we had in our bash examples, but in Python using default values: # we are also importing stderr from sys # this way, we can write errors to the error stream from sys import argv, stderr # parameters of the form \"name=value\" have default value # if some of your parameters do not have default values, # they must come before any parameters that do def print_error_and_exit(error_message=\"An error occurred\", exit_code=99): # file=stderr tells print to send output to STDERR instead of STDOUT print(error_message, file=stderr) print(f\"Usage: {argv[0]} [source_dir] [destination_dir]\", file=stderr) exit(exit_code) Conditionals Python provides a unified set of comparison operators for conditional statements. Below is a table of operators: Operator Meaning == equals != not equals < less than <= less than or equal to > greater than >= greater than or equal to Like bash, Python provides an if statement, although the syntax is different. Python if statements do not have a then , rather, they have a colon that follows the condition. Since Python uses indentation to mark blocks, there is no need for fi . When you return to the old indentation level, the interpreter knows the if has completed. Just like in bash, Python offers if , elif , and else . One thing that is more limited in Python than bash is the lack of operators for checking files and directories. There are instead pre-defined functions we can use for that, which are in the os and os.path modules. Below is a rewritten version of the bash script for copying all the files from one directory to another: from sys import os, stderr # many of the file checking functions are in the os module import os # the shutil module is for working on collections of files # we will use this to copy the contents of one directory to another from shutil import copytree # A Python documentation comment goes inside the function, as seen below def print_error_and_exit(error_message=\"An error occurred\", exit_code=99): \"\"\" print_error_and_exit - prints an error message and exits with the specified code error_message - the error message to be printed to stdout exit_code - the code to exit with \"\"\" print(error_message, file=stderr) print(f\"Usage: {argv[0]} [source_dir] [destination_dir]\", file=stderr) exit(exit_code) # here, we are checking the number of arguments # unlike in bash, this count WILL include the script name if len(argv) != 3: # we indent for if statements just like for functions print_error_and_exit(\"Invalid number of command line args\", 1) src_dir = argv[1] if not os.path.isdir(src_dir): print_error_and_exit(\"source_directory does not exist or is not a directory\", 2) dst_dir = argv[2] if not os.path.isdir(dst_dir): print_error_and_exit(\"destination_directory does not exist or is not a directory\", 3) # copytree will recursively copy all files from src_dir to dst_dir copytree(src_dir, dst_dir) exit(0) Loops Python has while and for loops, just like bash. Again, their syntax varies. Let's look at some examples: from sys import argv # python for loops can easiler iterate through lists # this for loop will print each command line argument for arg in argv: print(arg) # if you want to use numeric indexes to loop, you use the range function for i in range(0, len(argv)) print(f\"argv[{i}] = {argv[i]}\") # while loops use a condition sum = 0 val = float(input(\"Enter a number (0 to stop): \")) sum = sum + val while val != 0: val = float(input(\"Enter a number (0 to stop): \")) sum = sum + val print(f\"The final total is {sum}\") total = 0 count = 0 # the break statement can be used to escape loops # this loop will run infinitely until the user answers \"n\" # to the \"continue?\" question while True: val = float(input(\"Enter a number: \")) count = count + 1 total = total + val answer = input(\"Continue (y/n)?\") if answer == \"n\": break print(f\"The average of {count} numbers is {total / count}\") Containers Python offers several \"container\" types, which are used to store multiple values. We have seen list previously. We will now look at these in more detail. List Python's lists are like arrays in bash. However, they are far more consistent and useable in Python. Lists are collections of values, which can be accessed by their subscript, just like in bash. List elements can have any data type, and a list can contain elements of multiple data types. Below are some examples of lists: # an empty list empty_list = [] # you can also create an empty list like this empty_list = list() # you can also provide initial values numbers = [ 1, 2, 3, 4, 5 ] # you can add additional elements using append numbers.append(6) # numbers is now [ 1, 2, 3, 4, 5, 6 ] # lists can contain anything kitchen_sink = [1, \"hello world\", [\"this\", \"is\", \"a\", \"list\", \"in\", \"a\", \"list\"]] # if you have a very long list, it is allowed to span multiple lines long_list = [ 1, 2, \"the list is ended by the closing ]\", 12 ] Dictionary Python dictionaries ( dict ) are used to store key-value pairs. These let you associate a name, rather than a numeric index, to the data being stored: # we could use a list, but its not very descriptive student_info_list = [ \"Adam\", 21, 3.5 ] # how is anyone supposed to know that student_info_list[0] is the name? name = student_info_list[0] # a dictionary is a better choice, because we can label the elements # dictionaries use curly braces instead of square braces # each element is a key:value pair student_info_dict = { \"name\": \"Adam\", \"age\": 21, \"gpa\": 3.5} # now we can access the fields via their key name age = student_info_dict[\"age\"] Tuple A tuple is exactly like a list, except it cannot be modified after being created. Tuples are created using parenthesis () instead of square brackets []: a_list = [1, 2, 3] a_tuple = (1, 2, 3) # allowed a_list[0] = 0 # not allowed, this will trigger an error a_tuple[0] = 0 # also not allowed a_tuple.append(4) Set A set is similar to a list, but each element is unique. Sets use curly brackets {}, but do not have keys for their values: fruits = { \"apple\", \"orange\", \"banana\" } print(fruits) # prints: { \"apple\", \"orange\", \"banana\" } # apple will not be inserted since it already exists fruits.insert(\"apple\") print(fruits) # prints: { \"apple\", \"orange\", \"banana\" } List Comprehension One of the most popular constructs offered in Python is list comprehensions. Consider this snippet that creates a list with numbers from 1 to 100: numbers = [] for i in range(100): numbers.append(i + 1) Python allows us to write this in a single line: numbers = [i + 1 for i in range(100)] This is called a list comprehension, and you will find these often when looking up Python help, so they are worth mentioning. File I/O Python can be used to read and write the contents of files. Below is the typical syntax to open a file in Python: # the \"with\" statement automatically closes the file # the second argument of 'r' means the file is opened for reading with open(\"/path/to/file.txt\", 'r') as f: lines = f.readlines() # print lines to console for line in lines: print(line) # the 'w' means we are opening the file to write # like the \">\" redirection in bash, this will # overwrite the current contents of the file with open(\"hello.txt\") as f: # the b-string is bytes # it treats the text as ASCII/UTF-8 bytes # which is typically what we want for Unix text files f.write(b\"Hello, World!\") # the 'a' is for append # it is equivilent to the >> in bash # the original file content is kept # and the new text is added to the end with open(\"hello.txt\", 'a') as hello: hello.write(b\"Hello, again!\") Much more This is only scratching the surface of what can be done in Python. Again, if you want an interactive tutorial, check out W3Schools Python Tutorial to practice and learn more. Practice yourself by coming up with script ideas and doing the research to implement them. Regular Expressions and grep Regular expressions, abbreviated \"regex\", are used to search for patterns within text. Regex is a combination of text and meta-characters that form a search pattern. We can use grep to search for regex matches in text. grep will return lines within text that match the regex pattern. Say we have this text file: romeo_juliet.txt Two households, both alike in dignity (In fair Verona, where we lay our scene), From ancient grudge break to new mutiny, Where civil blood makes civil hands unclean. From forth the fatal loins of these two foes A pair of star-crossed lovers take their life; Whose misadventured piteous overthrows Doth with their death bury their parents\u2019 strife. The fearful passage of their death-marked love And the continuance of their parents\u2019 rage, Which, but their children\u2019s end, naught could remove, Is now the two hours\u2019 traffic of our stage; The which, if you with patient ears attend, What here shall miss, our toil shall strive to mend. The simplest regex is plain text with no meta-characters. Say we just wanted to find all lines that contain the word \"Verona\". We can search this with the command: grep Verona romeo_juliet.txt The output will be: (In fair Verona , where we lay our scene), The first meta-character we will look at is ^ . This checks for matches at the start of a line. The following search will find all lines that start with \"From\": grep \"^From\" romeo_juliet.txt Output: From ancient grudge break to new mutiny, From forth the fatal loins of these two foes The next meta-character is $ . This checks for matches at the end of the line. This search finds lines that end with a semicolon: grep \";$\" romeo_juliet.txt Output: A pair of star-crossed lovers take their life ; Is now the two hours\u2019 traffic of our stage ; The period . is a meta-character that matches any character. The following search will find all lines that contain a word that starts with any single letter followed be \"ove\". In this text file, the only word is \"love\", but this search could also find \"move\", for example. Note that there is a space in front of the period. This will create a slight issue with the search, as it will not find a match at the start of the line. grep \" .ove\" romeo_juliet.txt Output: A pair of star-crossed love rs take their life; The fearful passage of their death-marked love While we won't find any matches in the romeo_juliet.txt file, we can still fix up the search above to find words at the start of sentences that match the \".ove\" pattern. Just using \".ove\" won't work, since it will also find words like \"remove\", which we don't want for our arbitrary search. We only want words that have exactly one letter before \"ove\". We can use | to check multiple regex in one search. This allows us to add \"^.ove\" to also search the start of the line: # using | requires extended regex # therefore, add the -E switch for grep grep -E \"^.ove| .ove\" romeo_juliet.txt Square brackets are used to specify a character class. Often, this is done to allow for a lowercase or uppercase version of a letter. The following searches for the word \"the\" or \"The\". This search is also flawed, as it will also find words like \"there\", \"their\", etc.: grep \"[tT]he\" romeo_juliet.txt Output: From forth the fatal loins of the se two foes A pair of star-crossed lovers take the ir life; Doth with the ir death bury the ir parents\u2019 strife. The fearful passage of the ir death-marked love And the continuance of the ir parents\u2019 rage, Which, but the ir children\u2019s end, naught could remove, Is now the two hours\u2019 traffic of our stage; The which, if you with patient ears attend, Curly braces are used to specify a quantity of characters to match. You can specify an exact number of matches {n} , a minimum number of matches (the comma indicates it is a minimum and not exact search) {min,} , or a range {min,max} . The following search finds words that contain two Os, like \"look\", \"book\", \"shook\", \"took\", etc.: # we need extended regex for this # thus, we use the -E switch for grep grep -E \".o{2}.\" romeo_juliet.txt Output: Where civil b lood makes civil hands unclean. The hyphen - allows us to specify a range of characters. We can use the . to specify any character, but we may want to only allow letters or numbers, for example. We can refine the previous search to guarantee that \"oo\" is preceded by and followed by a letter and not a number or other character: # we need extended regex for this # thus, we use the -E switch for grep grep -E \"[a-z|A-Z]o{2}[a-z|A-Z]\" romeo_juliet.txt Parenthesis can be used to group together characters. The asterisk * can be used specify 0 to unlimited instances of a character. The pattern bo* would match b , bo , bo , boo , booo , etc. The plus + can be used to specify 1 to unlimited instances of a character. The pattern boo+ would match boo , booo , boooo , etc. The question mark ? is used to specify 0 or 1 instances of the preceding character or pattern. https? will match http and https . The backslash \\ is used as an escape character. It lets you search for meta-characters. So, if you wanted to search for money, you can use backslash to escape the $: \\$[0-9]+.([0-9]{2}) The question mark can also be attached to the + and * meta-characters. The book explains greedy searching in detail. The brief explanation is that * and + will exhaust all matches. Then, the matches will be backed off to account for the rest of the pattern. An example is the string \"aaaaa\". The pattern a*a will check the a* first. It will match \"a\", \"aa\", \"aaa\", \"aaaa\", \"aaaaa\". There is no more text to search in the string, so that ends the search for a* . Now it searches for a after a* . Since there are no remaining characters to search, the final \"a\" in \"aaaaa\" is taken away from a* , and given to a to check. This is enough to satisfy the search. For longer strings and patterns, this can be very inefficient. The regex a*?a will match the first \"a\" to a* . Since this is enough to satisfy a* , it stops searching there. It then moves on to matching a , which the second a in \"aaaaa\" satisfies. Using Regex in Scripts In addition to grep searches, we can use regex in both bash and Python scripts. To use a regex comparison in bash, use the =~ operator in a conditional: #!/bin/bash # this script checks if the command line arg # is a valid format for an email address # note: this is a simplified check if [[ $1 =~ .+@.+\\..+ ]] ; then echo \"$1 is an email address\" else echo \"$1 is not an email address\" fi exit(0) Regex can also be used in Python. This is done via the re module. More information can be found here: https://docs.python.org/3/library/re.html. Below is a basic example of re being used for the same email checking task: from sys import argv import re if re.search(\".+@.+\\..+\", argv[1]): print(f\"{argv[1]\" is an email address}\") else: print(f\"{argv[1]\" is not an email address}\") exit(0) vi and vim vi was an editor available in Unix. vim is a newer editor based on vi that adds many features. On most modern Linux distributions, calling vi will actually invoke vim . We'll use vi to refer to vim from here on out. vi is a multi-modal editor. There is some debate as to how many modes it actually has; some people say two, others say three, some say even move. Most text editors and word processors people use in daily life have a single mode. You are always capable of editing text by typing in the text window, and can perform tasks like copy-pasting, saving, opening files, closing files, etc. via modifier key shortcuts (CTRL, ALT plus another key) or by using a mouse. Command line/terminal environments do not typically support mouse input, so everything must be done via the keyboard. This is the primary motivation behind vi 's multiple modes. I'll treat vi as having two modes. The two primary modes are: Normal: Used for navigation and editing. Press the escape key to enter normal mode. Insert: Used for editing text directly (typing). Press the \"i\" key while in normal mode to enter insert mode. There are some special commands in vi that start with a colon. Common ones are listed below: :q! : exit without saving :x! : save and exit :wq : save and exit :w filename : save to a specified filename :set option : sets an editor option . For example, :set number toggles line numbers on and :set nonumber turns them off :line_number : jumps to the specified line_number . For example, :10 will jump to the 10th line of the file :$ : jumps to the last line of the file :/<regex> : searches the file for the regex pattern. Pressing the \"n\" key takes you to the next match The normal mode has several options as well that do not start with a colon: - dd : deletes the selected line - u : undo - v : pressing v and using the arrow keys allow you to highlight sections of text - y : stands for \"yank\" mode. This copies the selected text to the clipboard - o : inserts a new line. Will also put you into insert mode - p : pastes text currently in the clipboard Insert mode is simple, you just type text. Just remember to press Escape to get out of insert mode and get back to normal mode. Here are some vi tutorials: - Basic with visuals: https://opensource.com/article/19/3/getting-started-vim - Basic by sysadmin: https://www.linuxfoundation.org/blog/blog/classic-sysadmin-vim-101-a-beginners-guide-to-vim - More in-depth: https://github.com/iggredible/Learn-Vim Remember: press escape to ensure that you are in normal mode, then :q! to exit without saving and :x! to save and exit, and you won't end up like this poor soul... Conclusion We reviewed basic shell commands before moving into shell scripts. Shell scripts allow us to put a sequence of shell commands in a file to be executed over and over. This allows us to automate repetitive tasks, and document that steps taken to complete them. Python and other higher level scripting languages like Perl, Ruby, Raku, etc. allow for more complex tasks to be completed. When a bash script is going to take more than 50 to 100 lines, or requires more complex data structures like arrays, you should consider writing a Python script instead. There is a lot to learn with Python, but it is a very powerful language, and is useful to learn. We then discussed regular expressions (regex), which allow us to specify patterns for searching through text. Regex can be used with grep , as well as in bash and Python scripts. Finally, we discussed the Vim editor.","title":"Weeks 2 and 3"},{"location":"NET264/week2/#net264-unixlinux-system-administration-weeks-2-and-3","text":"These two weeks will cover shell scripting, Python scripting, regular expressions, and the Vim editor as they pertain to being a Linux system admin. These weeks correspond to Chapter 7 in the 5th edition of the book, and Chapter 2 of the 4th edition book. For this class, you only need to worry about Python, and not Perl or Ruby which are mentioned in the book.","title":"NET264 - Unix/Linux System Administration: Weeks 2 and 3"},{"location":"NET264/week2/#introduction","text":"Scripting allows you to automate tasks that are boring and repetitive. This reduces the chance for error, and provides documentation for how these tasks are completed. The focus in this class is on smaller, day to day scripts you'd write as a sysadmin, rather than larger scale scripting for fully automated server deployment, for example. We'll cover both shell and Python scripting. Shell scripts are more portable, as they are typically very small and only invoke commands that are available on pretty much any Unix-like OS. Python scripting is much more powerful, but much of Python's power comes from its rich ecosystem of 3rd party libraries and modules, making it less portable across different systems without significant setup.","title":"Introduction"},{"location":"NET264/week2/#shell-basics","text":"This should mostly be a review of things you learned in NET220. But, if it's been a while, this can be a helpful refresher.","title":"Shell Basics"},{"location":"NET264/week2/#redirection-and-piping","text":"The most powerful component of command shells and shell scripting is redirection and piping. This allows commands to interact with each other's output. The shell has three communication channels it makes available to processes: STDOUT (standard output), STDIN (standard input), and STDERR (standard error output). Each open I/O channel is provided a file descriptor, a small integer than provides a way to reference open files. STDIN is always 0, STDOUT is always 1, and STDERR is always 2. In a typical interactive shell session, STDIN is read from the keyboard, and STDOUT and STDERR are written to the terminal on the computer monitor. The redirection operators (<, >, >>) allow you to change the destination of the STD streams. The < symbol allows a command's STDIN to be connected to a file. The > symbol redirects STDOUT from a command to a file. > will overwrite the original contents of the file. If you wish to append STDOUT to a file, you should use >> instead. To redirect STDOUT and STDERR to the same place, use &> or &>>. To redirect only STDERR, use 2> or 2>>. Redirecting STDERR can be useful when a command may produce many error messages that are not important. For example, if you are running the command find without sudo , you will get many \"Permission denied\" errors when it tries to search directories you do not have access to. To ignore these, you can use the file /dev/null to dump the output of STDERR: # searching for pdf files find -name '*.pdf' 2> /dev/null The above command will only report files that are located to the terminal. All the \"Permission denied\" errors are sent away to /dev/null. If you have many results, you may wish to store them into a file to be searched later. You can dump STDOUT and STDERR to separate locations: find -name '*.pdf' > PDFs.txt 2> /dev/null This will write all the PDF files located by find to a file called PDFs.txt, while the error messages are still discarded. What if you want to chain two commands together? The pipe operator, | , can be used for that. | will redirect the STDOUT of one command to the STDIN of the next. For example, I may want to find PDFs in my current directory. I can pipe the output of ls into grep : ls -la | grep -i .pdf This will display all files that contain '.pdf' in their name. Similar to these concepts, you can conditionally run commands based on the success/failure of the preceding command. The && operator can be used to only run a second command if the first succeeds. For example, cp /usr/share/shared.txt . && cat shared.txt will only run cat shared.txt if shared.txt was successfully copied from /usr/share . To run a command if the preceding command fails, use the || operator. You can use a \\ at the end of a line to continue the same command on the next line. To have multiple commands on the same line, separate them using ; .","title":"Redirection and Piping"},{"location":"NET264/week2/#variables","text":"Shell variables allow you to provide a name to access a value. For example, if you have a long directory name that you will be using frequently, you can store its value in a variable to make it easier to access. To create a variable, type the name of the variable, followed immediately by an equal sign and the value of the variable. You cannot have any spaces between the name, equal sign, and value. The value can be wrapped in quotes. If the value has any spaces in it, it must be wrapped in quotes. Below is an example: long_dir='/home/me/Documents/NET264/Week2/Materials/week2_notes.txt` To access the variable's value, prefix it with a dollar sign. Otherwise, the shell will interpret the name as a text literal and not a variable: echo long_dir # will print: long_dir echo $long_dir # will print: /home/me/Documents/NET264/Week2/Materials/week2_notes.txt You may wrap variable name's in curly braces {} to help distinguish them. While not always required, they can be helpful when the variable's value is part of a text string. For example: josh='Josh' # this will print \"Josh is short for Joshua\" # we need the curly braces to separate the # variable from the \"ua\" part echo \"$josh is short for ${josh}ua\" # we could also put it on the first $josh echo \"${josh} is short for ${josh}ua\" There aren't formal standards for naming shell variables, but all caps names typically suggest that an environment variable or configuration variable. For general scripting, stick to snake_case names, which are all lowercase, and separate words with underscores.","title":"Variables"},{"location":"NET264/week2/#quotes","text":"Different styles of quotes mean different things in the shell. Strings with double quotes will replace embedded variables with their values: name=\"World\" echo \"Hello ${name}!\" # prints: Hello World! Single quotes treat all of their contents as literal text: name=\"World\" echo \"Hello ${name}!\" # prints: Hello ${name}! Backticks allow you to embed shell commands within a double-quoted string: # pwd will be replaced with the current working directory, # and ls will be replaced with the files in that directory echo \"Directory listing for `pwd`: `ls`\"","title":"Quotes"},{"location":"NET264/week2/#filtering-commands","text":"cut : prints sections of its input lines sort : sort lines uniq : print unique lines (and their counts) wc : word count (along with lines and characters). Use the -l , -w , or -c to specify which to print tee : copy input to two places. Good if you need to send the same output to your terminal and a file head : reads the first n lines of a file tail : read the last n lines of a file grep : searches text for regular expression and returns results. We will discuss this more in-depth later","title":"Filtering Commands"},{"location":"NET264/week2/#shell-scripting","text":"These instructions are for the GNU Bourne-Again SHell (bash), but will apply to most other command shells like dash, zsh, etc. It will not apply to more programming language-like shells such as csh and tcsh. bash is good for smaller scripts (50 to 100 lines max). Writing bash scripts is basically like taking the commands you'd write in the shell, and putting them in a file to be run all at once. The main way to develop bash scripts is to test the commands you need to run in the terminal. Once you are satisfied, you can write them out in your script. Like in the terminal, comments in bash start with the pound symbol # . You can redirect and pipe the output of commands just as you would in the normal terminal shell. Below is a simple bash script to write \"Hello World!\" to the console. helloworld #!/bin/bash echo \"Hello World!\" The first line is a \"shebang\" comment, and indicates to the OS kernel that this file is meant to be run by the program /bin/bash . We could alternatively put /bin/zsh if we wanted to use zsh to run this script. We can even write scripts for Python by putting #!/bin/python3 , although bash scripts are not valid Python and the interpreter would crash. There multiple options to run this script. As a plain text file, we can call the shell interpreter we want to use and pass it the script as a command line argument: bash helloworld This will create a new instance of bash to run the command that will immediately end when the script has completed. Using the source command will run the script in your current shell session: source helloworld Neither of these options require the script file to be executable, and will ignore the shebang comment at the top. If we want to run the script directly, like we would a program, we must first make it executable using the chmod command: chmod +x helloworld ./helloworld In this case, the OS will read the shebang comment at the top of the file, and use that command interpreter to execute the script. If there is no shebang comment, the OS will use the shell that the script was run from to execute it. You may notice that the helloworld script has no file extension. Unix-like operating systems do not use extensions to identify file types like Windows does. With that being said, shell scripts often have the extension .sh to indicate they are scripts, and to differentiate them from directories and executable binaries.","title":"Shell Scripting"},{"location":"NET264/week2/#building-scripts","text":"Often, we would like to test our commands before hiding them away in a script to be run indiscriminately in the future. We build up scripts by running our desired commands in the terminal, then, when satisfied, copy the commands into our script file. To avoid making potentially unwanted changes when testing the script, you can use echo statements to print the commands you plan on running, rather than executing them. This way, you can check the value of variables and how they are formatted within the commands before running the command.","title":"Building Scripts"},{"location":"NET264/week2/#inputoutput","text":"echo and printf are the main commands for writing output text. printf is more powerful, but also more finicky to use in some cases. Input can be read from STDIN using the read command. The script below will ask the user for their name and print it back out to them. echo -n \"What is your name?\" read name echo \"Your name is ${name}.\" exit 0 This is okay, but what if the user types nothing and hits enter? We can check that the input is not blank using an if statement. We will cover if more in-depth later, but for now it should be obvious what its purpose is: echo -n \"What is your name?\" read name # -n \"${name}\" is true if the value of name is not null if [ -n \"${name}\" ] ; then echo \"Your name is ${name}.\" exit 0 else # >&2 redirects output to STDERR, since it is an error message echo \"You forgetting something...?\" >&2 exit 1 fi","title":"Input/Output"},{"location":"NET264/week2/#command-line-arguments","text":"Typically, scripts do not get interactive user input. Often, information is passed into scripts via \"command-line arguments\". These are values that you provide to the script when you run it. Let's consider this script below that copies the contents of a source directory to a target. It is using interactive user input to get the paths of the two directories: copy_directory_contents.sh echo -n \"Enter source directory: \" read src_dir # the -d checks if the value of src_dir is a directory # the ! out in front negates it, so our check is # if src_dir is not a directory if ! [ -d $src_dir ] ; then echo \"${src_dir} is not a directory...\" >&2 exit 1 fi echo -n \"Enter target directory: \" read dst_dir # same check for dst_dir if ! [ -d $dst_dir ] ; then echo \"${dst_dir} is not a directory...\" >&2 exit 2 fi cp -r src_dir/* dst_dir/ exit 0 Let's rewrite this script so that we use command-line arguments to specify the directories. Command-line arguments are named $0 , $1 , $2 , etc. for each argument. $0 is always the command used to run the script. $1 is the first command-line argument, $2 is the second, etc. The variable $# is the number of command line arguments, not counting $0 . So, if we ran ./copy_directory_contents.sh /my/source/directory /my/target/directory , the value of $0 would be ./copy_directory_contents.sh , $1 would be /my/source/directory , and $2 would be /my/target/directory . The value of $# would be 2. With this in mind, let's write the modified script that uses command-line arguments: copy_directory_contents.sh # -ne is not equal # this first check verifies if there are 2 command line args if [ 2 -ne $# ] ; then echo \"Must provide source and target directory\" >&2 echo \"Usage: $0 [source_directory] [target_directory]\" >&2 exit 1 fi src_dir=$1 if ! [ -d $src_dir ] ; then echo \"${src_dir} is not a directory...\" >&2 exit 2 fi dst_dir=$2 if ! [ -d $dst_dir ] ; then echo \"${dst_dir} is not a directory...\" >&2 exit 3 fi cp -r src_dir/* dst_dir/ exit 0 The script first ensures that there are two command-line arguments. The -ne checks for not equal, so if $# is not equal to 2, an error message is printed and the script aborts. The bash variable src_dir is given the value of the first command-line argument, $1 , and the variable dst_dir is given the value of the second, $2 . Error messages are printed if either is not a directory.","title":"Command-Line Arguments"},{"location":"NET264/week2/#functions","text":"Perhaps we'd also like to print the usage message each time there is an error. We could copy the echo statement that contains the script's usage into each if statement, but that means that we have to remember to update the usage message in all three places if we change how the script should be used in the future. When there is a sequential set of commands that you want to run in multiple places, you can write a function to do so. A function is like a user-defined command. Anywhere you put the name of the function (often referred to as \"calling the function\" or \"invoking the function\"), you can think of it as the commands within the function being copy-pasted to that spot. function show_usage { echo \"Usage: $0 [source_directory] [target_directory]\" >&2 } if [ 2 -ne $# ] ; then echo \"Must provide source and target directory\" >&2 show_usage exit 1 fi src_dir=$1 if ! [ -d $src_dir ] ; then echo \"${src_dir} is not a directory...\" >&2 show_usage exit 2 fi dst_dir=$2 if ! [ -d $dst_dir ] ; then echo \"${dst_dir} is not a directory...\" >&2 show_usage exit 3 fi cp -r src_dir/* dst_dir/ exit 0 Just like you can pass arguments to scripts and commands, you can also pass arguments to functions. A function's arguments are accessed exactly like command-line arguments to the script. If you are going to have a function that takes arguments, a comment explaining the arguments and their expected order should be provided. Let's modify the script again to use a function to handle printing all the script's error messages and aborting: # print_error_and_exit [error_message] [exit_code] # error_message - message printed before the usage # exit_code - code to return to OS when exiting script function print_error_and_exit { # checking if custom error message is provided if [ -n $1 ] ; then echo $1 >&2 else # printing default error message if none provided echo \"An error ocurred\" fi echo \"Usage: $0 [source_directory] [target_directory]\" >&2 # checking if error code is provided if [ -n $2 ] ; then exit $2 else # exit with non-zero code exit 99 fi } if [ 2 -ne $# ] ; then print_error_and_exit \"Must provide source and target directory\" 1 fi src_dir=$1 if ! [ -d $src_dir ] ; then print_error_and_exit \"${src_dir} is not a directory...\" 2 fi dst_dir=$2 if ! [ -d $dst_dir ] ; then print_error_and_exit \"${dst_dir} is not a directory...\" 3 fi cp -r src_dir/* dst_dir/ exit 0","title":"Functions"},{"location":"NET264/week2/#variable-scope","text":"All variables in bash are global, meaning their value is visible and changeable in all locations of the script. However, functions can temporarily claim a variable's name inside itself by using local to localize the name. Otherwise, variables can have their values overwritten by the function. These two example scripts will illustrate this: no_local.sh function foo { echo \"x at start of function = ${x}\" x=\"world hello\" echo \"x at end of function = ${x}\" } x=\"hello world\" echo \"x before function = ${x}\" foo echo \"x after function = ${x}\" Output: x before function = hello world x at start of function = hello world x at end of function = world hello x after function = world hello As you can see, after the function was called, x has a different value. If you want to prevent variables within the function from impacting the script outside the function, you should declare them as local : local.sh function foo { echo \"x at start of function = ${x}\" local x=\"world hello\" echo \"x at end of function = ${x}\" } x=\"hello world\" echo \"x before function = ${x}\" foo echo \"x after function = ${x}\" Output: x before function = hello world x at start of function = hello world x at end of function = world hello x after function = hello world","title":"Variable Scope"},{"location":"NET264/week2/#conditional-statements","text":"We saw if statements earlier. Now, let's take a look at them more in depth. As the name would suggest, the commands within an if statement only execute if the condition that is being checked is true. Optionally, the if may contain an else that indicates what do if the condition is false. To check multiple conditions, elif can be used after the first if , but before any else . An if statement can only have one if at the beginning, and one else at the end. You can put as many elif s between as you would like. If statements always start with the word if , and end with fi , which comes after all elif and else blocks. The if and elif keywords are followed by conditions , which are tested to be true or false . Conditions are almost always wrapped in square brackets: [ condition ], which is inherited from the original Bourne shell. Some people wrap bash conditions in two sets of square brackets: [[ condition ]]. This is not guaranteed to be portable to shells other than bash, however.","title":"Conditional Statements"},{"location":"NET264/week2/#operators","text":"Below is a table of comparison operators for bash variables. Symbols ( = , < , > , etc.) are used to compare values as text strings, while text \"operators\" ( -eq , -ne , etc.) are used to compare the values of variables as numbers. String Number English Interpretation x = y x -eq y x is equal to y x != y x -ne y x is not equal to y x < y x -lt y x is less than y x -le y x is less than or equal to y x > y x -gt y x is greater than y x -ge y x is greater than or equal to y -n x x is not null (null means empty) -z x x is null There are also special operators to treat the value being tested as a file path: | Operator | Interpretation | | -e path | path exists | | -d path | path is a directory | | -f path | path is a file | | -s path | path exists and is not empty | | -r path | You have read permission for path | | -w path | You have write permission for path | | path1 -nt path2 | path1 is newer than path2 | | path1 -ot path2 | path1 is older than path2 | A condition can be negated by putting an exclamation point ! before the condition. Now that we have the basics, let's write a basic script using if , elif , and else . This script will take a path as its argument, and determine if it is a file, directory, or empty. #!/bin/bash path=$1 # using -z to check if path is null if [[ -z $path ]] ; then echo \"Need to provide path\" >&2 exit(1) # -e path checks if it exists # putting ! before the square brackets negates it elif ! [[ -e $path ]] ; then echo \"$path does not exist\" # -f checks if the path is to a file elif [[ -f $path ]] ; then echo \"$path is a file\" # at this point, the path exists and is not a file # therefore, it must be a directory else echo \"$path is a directory\" fi exit(0)","title":"Operators"},{"location":"NET264/week2/#loops","text":"Loops allow a set of instructions to be repeated. There are two types of loop in bash, for and while . for loops start with the keyword for followed by a control. Afterwards, you need the word do . Typically, a semicolon is used to allow the do to be placed on the same line as for . The loop is ended with the word done . Otherwise, do has to go on its own line. You can create the typical for loop you'd see in programming languages like C: for ((i=0; i<$MAX; i++)); do echo \"$i\" done Often, though, bash for-loops are used to iterate through file globs. Hopefully you remember from your previous Linux class that you can use wildcards like * to create a basic pattern. Something like *.pdf would be a collection of all files that end with \".pdf\". Below is a for loop that finds all files named with \".PDF\" and renames them to \".pdf\". for $f in *.PDF ; do mv \"$f%.PDF\" \"$f.pdf\" done While loops can be used when the number of times the loop must run is indeterminate. The syntax is otherwise the same as a for loop. Below is an example of reading the lines of a file and printing them to the console: # reads the content of the first command line arg # into STDIN exec 0<$1 line_number=1 # read command reads from STDIN until newline is found # since the file is in STDIN, this effectively reads # a line from the file into the variable line while read line; do echo \"$line_number $line\" $((counter++)) done","title":"Loops"},{"location":"NET264/week2/#arithmetic","text":"You've seen in a couple places in the previous sections that there have been cases where bash variables have been wrapped in two sets of parenthesis. For example, the $((counter++)) in the previous example. As we've discussed, all bash variables are treated as text strings. So, what if we need to treat them as numbers. That is where the double parenthesis come in. When a bash variable is wrapped in double parenthesis, it is treated as a number instead of a string. This doesn't just apply to variables; raw numbers need to be wrapped in parentheses too if you want them to be treated as numbers. #!/bin/bash # arith.sh x=1 y=$((2)) z=$x+$y a=$((x+y)) # even if using a number, still need (()) b=$((x+1)) echo $z # prints: 1+2 echo $a # prints: 3 echo $b # prints: 2","title":"Arithmetic"},{"location":"NET264/week2/#arrays","text":"The last bash topic we'll cover is arrays. Arrays are a collection of values stored under a single name. The individual values, often called elements, are accessed via a subscript (also called index). Below is an example of a bash array, and how to access its elements: # arrays are declared like normal variables, # but the elements are wrapped by parentheses # elements are separated by spaces array=(a b c d e f g) array_size=7 # the first index in an array is 0 for ((i=0; i<$array_size; i++)) ; do # individual elements are accessed using # square brackets with the desired index # within them echo \"${array[$i]}\" done # you can also update the values array[0]=A There are several peculiarities with bash arrays, which are detailed in the book. In general, if your script is complicated enough that you need to use an array, you might be better served writing it in Python...","title":"Arrays"},{"location":"NET264/week2/#python","text":"Released in 1991, Python was created by Guido van Rossum (aka the Benevolent Dictator for Life). The current version, python3 , comes preinstalled on most Linux distributions. A rather ironic line from the 4th edition of the book is \"This section describes Python 2. Python 3 is in the works and is likely to be released during the lifetime of this book. But unlike Perl 6, it appears likely to be a relatively incremental update.\" While it is true that Perl 6 is very different to Perl 5 (so much so that it is now called Raku), Python 3 is perhaps the most famous and extreme example of a programming language totally breaking backwards compatibility with its previous major release. With that said, the world has moved on, and python3 is Python, with Python 2 support ending entirely in 2020. However, the executable is still called python3 on many systems. In fact, that there is a popular package called python-is-python3 that exists solely to create a symlink to allow python3 to be run using the python command. With our trip down memory lane out of the way, let's get into the meat and potatoes of Python. Python is a powerful scripting and general purpose programming language. There is a large community of official and third-party libraries and modules to support programming and scripting in Python. This is a blessing and a curse. This ecosystem allows for you to write powerful scripts quickly, but you may also fall into what is known as \"dependency hell\", where your script needs many software libraries to be installed alongside it to function correctly. Worse yet, you often need the correct versions of these libraries, otherwise they may not play nicely with each other. Luckily, the scripts we write in this class will not be using any third-party libraries. We will stick to only using libraries that are installed alongside Python. For a general tutorial on Python, check out https://www.w3schools.com/python/.","title":"Python"},{"location":"NET264/week2/#where-to-write-python","text":"There are several options to write Python. First, you can start an interactive shell session by typing python or python3 into a terminal. This will switch your bash shell to a Python shell. You can write Python statements after the >>> , which is called a prompt: Python from the shell You can exit the shell by typing exit() , quit() , or CTRL + D. If you are using a Linux distribution that includes a desktop GUI, you can run idle from the terminal to launch the Python Integrated Development and Learning Environment. If it is not installed, you will likely be given the instruction to do so. On Ubuntu, you should run sudo apt install idle . idle opens up a window with an interactive Python shell environment. You also have the option create and open Python script files from the \"File\" tab in the top left corner. The IDLE editor provides some limited syntax highlighting and tooltips. The next option is to write your scripts directly in a text file. If you wish to run your Python scripts like a bash script, using the ./script_name syntax, you will need to include a shebang comment at the top: #!/bin/python3 . Python files are often run using the interpreter, like: python3 script_name . If you only plan to run your scripts using python3 , you do not need the shebang. Python scripts typically have the .py extension to indicate that they are Python. Like bash, statements in Python are ended by a line return. Also like bash, if you want to put multiple statements on the same line, you can separate them using a semicolon.","title":"Where to Write Python"},{"location":"NET264/week2/#variables_1","text":"Python variables are much easier than bash. There is no more attaching dollar signs to the name, nor the need to have no spaces in the original declaration. A variable in Python can be named using any combination of letters, numbers, and the underscore character, however, it must not start with a number. Most Python programmers use snake_case for their variable names, just as we saw with bash. Variables whose values are not meant to be changed anywhere in the script are written using SCREAMING_SNAKE_CASE . Unlike bash, where all variables are treated as text strings, Python variables have a data type associated with them. Python is dynamically typed, meaning the interpreter infers the type of the variable; you do not need to provide it. Python has four primitive types: int (whole numbers), float (decimal numbers), bool ( True and False ), and str (any text string). Below is an example of some Python variables: # comments in Python start with a #, just like in bash x = 2 # since 2 is a whole number, x is an int y = 2.2 # since 2.2 is a decimal number, y is a float z = True # z is a bool s = \"Some text\" # s has the str (string) type","title":"Variables"},{"location":"NET264/week2/#output-and-input","text":"Python provides the print function to write output to the terminal. Print automatically adds a new line to the end. # Text in Python must always use quotation marks print(\"Hello World!\") print('Hello World!') x = 1 y = 2 z = 3 # commmas can be used to print multiple values # the values will be separated by spaces print(x, y, z) # prints: 1 2 3 # you can use the \"sep=\" argument to change the separator print(x, y, z, sep=\"\") # prints: 123 # to change the line end, you can use the \"end=\" argument print(\"hello\", end=\", \") print(\"world\", end=\"!\\n\")# together, these print: hello, world! Input is read using the input function. Optionally, you may provide a message to be printed when the input function prompts for input. To get the result of the function, you must set a variable equal to it. Note: the input function always treats input as a string. Even if I enter \"5\", it will not treat it as an int . I must explicitly cast the result as an int . See the example: # a name is a text string, # so we don't need to convert it to anything else name = input(\"Enter your name: \") print(\"Your name is\", name) # wrap input with int() to cast input as int age = int(input(\"Enter your age (in whole years): \")) print(\"Your age is\", age) # the same goes for float gpa = float(input(\"Enter your GPA: \")) print(\"Your GPA is\", gpa)","title":"Output and Input"},{"location":"NET264/week2/#command-line-arguments_1","text":"Python scripts can use command-line arguments, just like bash scripts can. However, they are accessed differently. To access the command-line arguments, we will need to introduce our first library/module. The sys module contains functions and objects that relate to Operating System. This includes argv , which is a list containing the command line arguments. A Python list is like a bash array, only much more powerful and useable. Below is some code to read the command-line arguments: # student_info.py # this imports the argv list from the sys module from sys import argv # like bash, the first command-line argument # is the command that ran the script script = argv[0] # assuming we have the same input as the # last example, but using command-line args name = argv[1] age = int(argv[2]) gpa = float(argv[3]) print(\"The script you ran is:\", script) print(\"Your name is\", name) print(\"Your age is\", age) print(\"Your GPA is\", gpa) We can run this script with: python3 student_info.py Adam 21 3.5 The output of the script would be: The script you ran is: student_info.py Your name is Adam Your age is 21 Your GPA is 3.5 Of course, we should also have all the error checking and handling we did for the bash scripts. I've skipped it to keep the examples shorter and less cluttered.","title":"Command-Line Arguments"},{"location":"NET264/week2/#formatted-strings","text":"In bash, we are able to inject variables directly into double-quoted strings. This is mainly thanks to the fact that they start with a dollar sign, allowing the bash interpreter easily to find and replace the variables with their actual value. Since Python does not use special characters to mark its variables, we cannot directly put variables into strings. We can get a similar effect by using formatted (also called interpolated) strings, often called f-strings in Python. An f-string has the letter f directly in front of the double-quotes. Inside the string, we can place curly braces anywhere we want to insert a variable. Let's rewrite the previous script using f-strings to show how this works: # student_info.py # this imports the argv list from the sys module from sys import argv # like bash, the first command-line argument # is the command that ran the script script = argv[0] # assuming we have the same input as the # last example, but using command-line args name = argv[1] age = int(argv[2]) gpa = float(argv[3]) print(f\"The script you ran is: {script}\") print(f\"Your name is {name}\") print(f\"Your age is {age}\") print(f\"Your GPA is {gpa}\")","title":"Formatted Strings"},{"location":"NET264/week2/#functions_1","text":"Python's functions are much like bash functions in terms of where you'd use them, however, they are declared and implemented differently. Python functions start with def , which is followed by the name of the function, its parameter list (which may be empty), and a colon. Here, we get to one of the peculiarities of Python: significant whitespace. In bash, the start and end of the function was specified using curly braces. Any commands between the opening and closing brace was considered to be part of the function. Python does not use curly braces to delineate blocks. Instead, it uses indentation. See the example below: # def is the keyword that tells python3 this is a function # add_and_print is the name of the function # x and y are the parameters # parameters are the names of the function's arguments # Python does not use $1, $2, etc. to access function arguments # instead, you must provide them a name # parameters are separated by commmas # parameters must be placed within the parenthesis # if a function has no parameters, you must still include empty parenthesis # the parameter list is followed by a colon def add_and_print(x, y): # the statements within the function must be indented z = x + y print(f\"{x} + {y} = {z}\") # returning to the previous level of indentation exits the function body a = 1 b = 2 # the function can be called using variables # the arguments must go in parenthesis add_and_print(a, b) # you can also use literal values add_and_print(3, 4) # or a combination add_and_print(a, 6) The number of arguments provided when calling the function must match the number of parameters. What if we want parameters to be optional? For that, we can provide a default value. Let's rewrite the print_error_and_exit function we had in our bash examples, but in Python using default values: # we are also importing stderr from sys # this way, we can write errors to the error stream from sys import argv, stderr # parameters of the form \"name=value\" have default value # if some of your parameters do not have default values, # they must come before any parameters that do def print_error_and_exit(error_message=\"An error occurred\", exit_code=99): # file=stderr tells print to send output to STDERR instead of STDOUT print(error_message, file=stderr) print(f\"Usage: {argv[0]} [source_dir] [destination_dir]\", file=stderr) exit(exit_code)","title":"Functions"},{"location":"NET264/week2/#conditionals","text":"Python provides a unified set of comparison operators for conditional statements. Below is a table of operators: Operator Meaning == equals != not equals < less than <= less than or equal to > greater than >= greater than or equal to Like bash, Python provides an if statement, although the syntax is different. Python if statements do not have a then , rather, they have a colon that follows the condition. Since Python uses indentation to mark blocks, there is no need for fi . When you return to the old indentation level, the interpreter knows the if has completed. Just like in bash, Python offers if , elif , and else . One thing that is more limited in Python than bash is the lack of operators for checking files and directories. There are instead pre-defined functions we can use for that, which are in the os and os.path modules. Below is a rewritten version of the bash script for copying all the files from one directory to another: from sys import os, stderr # many of the file checking functions are in the os module import os # the shutil module is for working on collections of files # we will use this to copy the contents of one directory to another from shutil import copytree # A Python documentation comment goes inside the function, as seen below def print_error_and_exit(error_message=\"An error occurred\", exit_code=99): \"\"\" print_error_and_exit - prints an error message and exits with the specified code error_message - the error message to be printed to stdout exit_code - the code to exit with \"\"\" print(error_message, file=stderr) print(f\"Usage: {argv[0]} [source_dir] [destination_dir]\", file=stderr) exit(exit_code) # here, we are checking the number of arguments # unlike in bash, this count WILL include the script name if len(argv) != 3: # we indent for if statements just like for functions print_error_and_exit(\"Invalid number of command line args\", 1) src_dir = argv[1] if not os.path.isdir(src_dir): print_error_and_exit(\"source_directory does not exist or is not a directory\", 2) dst_dir = argv[2] if not os.path.isdir(dst_dir): print_error_and_exit(\"destination_directory does not exist or is not a directory\", 3) # copytree will recursively copy all files from src_dir to dst_dir copytree(src_dir, dst_dir) exit(0)","title":"Conditionals"},{"location":"NET264/week2/#loops_1","text":"Python has while and for loops, just like bash. Again, their syntax varies. Let's look at some examples: from sys import argv # python for loops can easiler iterate through lists # this for loop will print each command line argument for arg in argv: print(arg) # if you want to use numeric indexes to loop, you use the range function for i in range(0, len(argv)) print(f\"argv[{i}] = {argv[i]}\") # while loops use a condition sum = 0 val = float(input(\"Enter a number (0 to stop): \")) sum = sum + val while val != 0: val = float(input(\"Enter a number (0 to stop): \")) sum = sum + val print(f\"The final total is {sum}\") total = 0 count = 0 # the break statement can be used to escape loops # this loop will run infinitely until the user answers \"n\" # to the \"continue?\" question while True: val = float(input(\"Enter a number: \")) count = count + 1 total = total + val answer = input(\"Continue (y/n)?\") if answer == \"n\": break print(f\"The average of {count} numbers is {total / count}\")","title":"Loops"},{"location":"NET264/week2/#containers","text":"Python offers several \"container\" types, which are used to store multiple values. We have seen list previously. We will now look at these in more detail.","title":"Containers"},{"location":"NET264/week2/#list","text":"Python's lists are like arrays in bash. However, they are far more consistent and useable in Python. Lists are collections of values, which can be accessed by their subscript, just like in bash. List elements can have any data type, and a list can contain elements of multiple data types. Below are some examples of lists: # an empty list empty_list = [] # you can also create an empty list like this empty_list = list() # you can also provide initial values numbers = [ 1, 2, 3, 4, 5 ] # you can add additional elements using append numbers.append(6) # numbers is now [ 1, 2, 3, 4, 5, 6 ] # lists can contain anything kitchen_sink = [1, \"hello world\", [\"this\", \"is\", \"a\", \"list\", \"in\", \"a\", \"list\"]] # if you have a very long list, it is allowed to span multiple lines long_list = [ 1, 2, \"the list is ended by the closing ]\", 12 ]","title":"List"},{"location":"NET264/week2/#dictionary","text":"Python dictionaries ( dict ) are used to store key-value pairs. These let you associate a name, rather than a numeric index, to the data being stored: # we could use a list, but its not very descriptive student_info_list = [ \"Adam\", 21, 3.5 ] # how is anyone supposed to know that student_info_list[0] is the name? name = student_info_list[0] # a dictionary is a better choice, because we can label the elements # dictionaries use curly braces instead of square braces # each element is a key:value pair student_info_dict = { \"name\": \"Adam\", \"age\": 21, \"gpa\": 3.5} # now we can access the fields via their key name age = student_info_dict[\"age\"]","title":"Dictionary"},{"location":"NET264/week2/#tuple","text":"A tuple is exactly like a list, except it cannot be modified after being created. Tuples are created using parenthesis () instead of square brackets []: a_list = [1, 2, 3] a_tuple = (1, 2, 3) # allowed a_list[0] = 0 # not allowed, this will trigger an error a_tuple[0] = 0 # also not allowed a_tuple.append(4)","title":"Tuple"},{"location":"NET264/week2/#set","text":"A set is similar to a list, but each element is unique. Sets use curly brackets {}, but do not have keys for their values: fruits = { \"apple\", \"orange\", \"banana\" } print(fruits) # prints: { \"apple\", \"orange\", \"banana\" } # apple will not be inserted since it already exists fruits.insert(\"apple\") print(fruits) # prints: { \"apple\", \"orange\", \"banana\" }","title":"Set"},{"location":"NET264/week2/#list-comprehension","text":"One of the most popular constructs offered in Python is list comprehensions. Consider this snippet that creates a list with numbers from 1 to 100: numbers = [] for i in range(100): numbers.append(i + 1) Python allows us to write this in a single line: numbers = [i + 1 for i in range(100)] This is called a list comprehension, and you will find these often when looking up Python help, so they are worth mentioning.","title":"List Comprehension"},{"location":"NET264/week2/#file-io","text":"Python can be used to read and write the contents of files. Below is the typical syntax to open a file in Python: # the \"with\" statement automatically closes the file # the second argument of 'r' means the file is opened for reading with open(\"/path/to/file.txt\", 'r') as f: lines = f.readlines() # print lines to console for line in lines: print(line) # the 'w' means we are opening the file to write # like the \">\" redirection in bash, this will # overwrite the current contents of the file with open(\"hello.txt\") as f: # the b-string is bytes # it treats the text as ASCII/UTF-8 bytes # which is typically what we want for Unix text files f.write(b\"Hello, World!\") # the 'a' is for append # it is equivilent to the >> in bash # the original file content is kept # and the new text is added to the end with open(\"hello.txt\", 'a') as hello: hello.write(b\"Hello, again!\")","title":"File I/O"},{"location":"NET264/week2/#much-more","text":"This is only scratching the surface of what can be done in Python. Again, if you want an interactive tutorial, check out W3Schools Python Tutorial to practice and learn more. Practice yourself by coming up with script ideas and doing the research to implement them.","title":"Much more"},{"location":"NET264/week2/#regular-expressions-and-grep","text":"Regular expressions, abbreviated \"regex\", are used to search for patterns within text. Regex is a combination of text and meta-characters that form a search pattern. We can use grep to search for regex matches in text. grep will return lines within text that match the regex pattern. Say we have this text file: romeo_juliet.txt Two households, both alike in dignity (In fair Verona, where we lay our scene), From ancient grudge break to new mutiny, Where civil blood makes civil hands unclean. From forth the fatal loins of these two foes A pair of star-crossed lovers take their life; Whose misadventured piteous overthrows Doth with their death bury their parents\u2019 strife. The fearful passage of their death-marked love And the continuance of their parents\u2019 rage, Which, but their children\u2019s end, naught could remove, Is now the two hours\u2019 traffic of our stage; The which, if you with patient ears attend, What here shall miss, our toil shall strive to mend. The simplest regex is plain text with no meta-characters. Say we just wanted to find all lines that contain the word \"Verona\". We can search this with the command: grep Verona romeo_juliet.txt The output will be: (In fair Verona , where we lay our scene), The first meta-character we will look at is ^ . This checks for matches at the start of a line. The following search will find all lines that start with \"From\": grep \"^From\" romeo_juliet.txt Output: From ancient grudge break to new mutiny, From forth the fatal loins of these two foes The next meta-character is $ . This checks for matches at the end of the line. This search finds lines that end with a semicolon: grep \";$\" romeo_juliet.txt Output: A pair of star-crossed lovers take their life ; Is now the two hours\u2019 traffic of our stage ; The period . is a meta-character that matches any character. The following search will find all lines that contain a word that starts with any single letter followed be \"ove\". In this text file, the only word is \"love\", but this search could also find \"move\", for example. Note that there is a space in front of the period. This will create a slight issue with the search, as it will not find a match at the start of the line. grep \" .ove\" romeo_juliet.txt Output: A pair of star-crossed love rs take their life; The fearful passage of their death-marked love While we won't find any matches in the romeo_juliet.txt file, we can still fix up the search above to find words at the start of sentences that match the \".ove\" pattern. Just using \".ove\" won't work, since it will also find words like \"remove\", which we don't want for our arbitrary search. We only want words that have exactly one letter before \"ove\". We can use | to check multiple regex in one search. This allows us to add \"^.ove\" to also search the start of the line: # using | requires extended regex # therefore, add the -E switch for grep grep -E \"^.ove| .ove\" romeo_juliet.txt Square brackets are used to specify a character class. Often, this is done to allow for a lowercase or uppercase version of a letter. The following searches for the word \"the\" or \"The\". This search is also flawed, as it will also find words like \"there\", \"their\", etc.: grep \"[tT]he\" romeo_juliet.txt Output: From forth the fatal loins of the se two foes A pair of star-crossed lovers take the ir life; Doth with the ir death bury the ir parents\u2019 strife. The fearful passage of the ir death-marked love And the continuance of the ir parents\u2019 rage, Which, but the ir children\u2019s end, naught could remove, Is now the two hours\u2019 traffic of our stage; The which, if you with patient ears attend, Curly braces are used to specify a quantity of characters to match. You can specify an exact number of matches {n} , a minimum number of matches (the comma indicates it is a minimum and not exact search) {min,} , or a range {min,max} . The following search finds words that contain two Os, like \"look\", \"book\", \"shook\", \"took\", etc.: # we need extended regex for this # thus, we use the -E switch for grep grep -E \".o{2}.\" romeo_juliet.txt Output: Where civil b lood makes civil hands unclean. The hyphen - allows us to specify a range of characters. We can use the . to specify any character, but we may want to only allow letters or numbers, for example. We can refine the previous search to guarantee that \"oo\" is preceded by and followed by a letter and not a number or other character: # we need extended regex for this # thus, we use the -E switch for grep grep -E \"[a-z|A-Z]o{2}[a-z|A-Z]\" romeo_juliet.txt Parenthesis can be used to group together characters. The asterisk * can be used specify 0 to unlimited instances of a character. The pattern bo* would match b , bo , bo , boo , booo , etc. The plus + can be used to specify 1 to unlimited instances of a character. The pattern boo+ would match boo , booo , boooo , etc. The question mark ? is used to specify 0 or 1 instances of the preceding character or pattern. https? will match http and https . The backslash \\ is used as an escape character. It lets you search for meta-characters. So, if you wanted to search for money, you can use backslash to escape the $: \\$[0-9]+.([0-9]{2}) The question mark can also be attached to the + and * meta-characters. The book explains greedy searching in detail. The brief explanation is that * and + will exhaust all matches. Then, the matches will be backed off to account for the rest of the pattern. An example is the string \"aaaaa\". The pattern a*a will check the a* first. It will match \"a\", \"aa\", \"aaa\", \"aaaa\", \"aaaaa\". There is no more text to search in the string, so that ends the search for a* . Now it searches for a after a* . Since there are no remaining characters to search, the final \"a\" in \"aaaaa\" is taken away from a* , and given to a to check. This is enough to satisfy the search. For longer strings and patterns, this can be very inefficient. The regex a*?a will match the first \"a\" to a* . Since this is enough to satisfy a* , it stops searching there. It then moves on to matching a , which the second a in \"aaaaa\" satisfies.","title":"Regular Expressions and grep"},{"location":"NET264/week2/#using-regex-in-scripts","text":"In addition to grep searches, we can use regex in both bash and Python scripts. To use a regex comparison in bash, use the =~ operator in a conditional: #!/bin/bash # this script checks if the command line arg # is a valid format for an email address # note: this is a simplified check if [[ $1 =~ .+@.+\\..+ ]] ; then echo \"$1 is an email address\" else echo \"$1 is not an email address\" fi exit(0) Regex can also be used in Python. This is done via the re module. More information can be found here: https://docs.python.org/3/library/re.html. Below is a basic example of re being used for the same email checking task: from sys import argv import re if re.search(\".+@.+\\..+\", argv[1]): print(f\"{argv[1]\" is an email address}\") else: print(f\"{argv[1]\" is not an email address}\") exit(0)","title":"Using Regex in Scripts"},{"location":"NET264/week2/#vi-and-vim","text":"vi was an editor available in Unix. vim is a newer editor based on vi that adds many features. On most modern Linux distributions, calling vi will actually invoke vim . We'll use vi to refer to vim from here on out. vi is a multi-modal editor. There is some debate as to how many modes it actually has; some people say two, others say three, some say even move. Most text editors and word processors people use in daily life have a single mode. You are always capable of editing text by typing in the text window, and can perform tasks like copy-pasting, saving, opening files, closing files, etc. via modifier key shortcuts (CTRL, ALT plus another key) or by using a mouse. Command line/terminal environments do not typically support mouse input, so everything must be done via the keyboard. This is the primary motivation behind vi 's multiple modes. I'll treat vi as having two modes. The two primary modes are: Normal: Used for navigation and editing. Press the escape key to enter normal mode. Insert: Used for editing text directly (typing). Press the \"i\" key while in normal mode to enter insert mode. There are some special commands in vi that start with a colon. Common ones are listed below: :q! : exit without saving :x! : save and exit :wq : save and exit :w filename : save to a specified filename :set option : sets an editor option . For example, :set number toggles line numbers on and :set nonumber turns them off :line_number : jumps to the specified line_number . For example, :10 will jump to the 10th line of the file :$ : jumps to the last line of the file :/<regex> : searches the file for the regex pattern. Pressing the \"n\" key takes you to the next match The normal mode has several options as well that do not start with a colon: - dd : deletes the selected line - u : undo - v : pressing v and using the arrow keys allow you to highlight sections of text - y : stands for \"yank\" mode. This copies the selected text to the clipboard - o : inserts a new line. Will also put you into insert mode - p : pastes text currently in the clipboard Insert mode is simple, you just type text. Just remember to press Escape to get out of insert mode and get back to normal mode. Here are some vi tutorials: - Basic with visuals: https://opensource.com/article/19/3/getting-started-vim - Basic by sysadmin: https://www.linuxfoundation.org/blog/blog/classic-sysadmin-vim-101-a-beginners-guide-to-vim - More in-depth: https://github.com/iggredible/Learn-Vim Remember: press escape to ensure that you are in normal mode, then :q! to exit without saving and :x! to save and exit, and you won't end up like this poor soul...","title":"vi and vim"},{"location":"NET264/week2/#conclusion","text":"We reviewed basic shell commands before moving into shell scripts. Shell scripts allow us to put a sequence of shell commands in a file to be executed over and over. This allows us to automate repetitive tasks, and document that steps taken to complete them. Python and other higher level scripting languages like Perl, Ruby, Raku, etc. allow for more complex tasks to be completed. When a bash script is going to take more than 50 to 100 lines, or requires more complex data structures like arrays, you should consider writing a Python script instead. There is a lot to learn with Python, but it is a very powerful language, and is useful to learn. We then discussed regular expressions (regex), which allow us to specify patterns for searching through text. Regex can be used with grep , as well as in bash and Python scripts. Finally, we discussed the Vim editor.","title":"Conclusion"},{"location":"NET264/week4/","text":"NET264 - Unix/Linux System Administration: Week 4 This week covers Chapters 2 and 3 of the 5th edition book, and Chapters 3 and 4 of the 4th edition book. The topics are Booting, Shut Down, Access Control, and Root elevation. Some details in the 4th edition book are out of date. Consult these notes for updates. Booting The boot process controls how the OS takes control of the computer's hardware when the computer is powered on. The boot process has evolved over the years, but in general it starts with a bootstrapping step, where the computer is powered on, and the hardware is responsible for finding and starting the operating system. Once the OS is loaded into memory, it must start the necessary programs, DLLs, and drivers to interact with the hardware. Finally, the OS can launch its login and hand control over to the user. Bootstrapping At this phase, the computer is highly vulnerable as it is dependent on its hardware to be functional, its configurations to be correct, and for the OS files to be uncorrupted and accessible. Despite the critical nature of bootstrapping and startup, sysadmins often have little control over the process. It is instead controlled by the manufacturer of the hardware. Some machines are purpose built to run UNIX or another proprietary operating system. Therefore, booting the targeted OS is simple. However, nowadays, many machines are PCs, which use BIOS (basic input/output system) or UEFI (unified extensible firmware interface). UEFI was designed to replace BIOS, and is the most commonly now. Colloquially, many people still call UEFI \"BIOS\", which is the convention I will follow here. If a remark pertains only to the original BIOS, it will be labeled as \"legacy BIOS\". Notes about UEFI will specifically refer to UEFI. Both are similar in that they are a very basic \"operating system\" that is installed on the computer's ROM (read-only memory). This is typically installed on the motherboard. The first thing that occurs when a computer's power switch is activated is the loading of BIOS to the computer's main memory to be executed on the CPU. BIOS's first task is the power-on self test (POST), where the system hardware is checked before loading the OS. BIOS allows you to set which devices to boot from, and in which order. BIOS will check for the existence of each device until it finds one to boot from. Legacy BIOS reads the first block of the device for the MBR (master boot record) to determine which partition to load the operating system bootloader from. UEFI reads the GUID Partition Table (GPT, no relation to the LLM) to determine where to boot. It does this by finding the special EFI partition, which has a special GUID (globally unique identifier). The EFI partition contains the GPT and the system bootloader files. At this point, the OS bootloader takes over and BIOS's job is done. GRUB Grand Unified Bootloader (GRUB) is the primary bootloader used by UNIX/Linux systems. Most distros use GRUB 2 now ( this contradicts a dated statement in the 4th edition book ), which is the successor to legacy GRUB. The primary differences are in the format and location of the configuration files, and that GRUB 2 allows the boot screen to be customized. From here on, GRUB means GRUB 2, and legacy GRUB will not be discussed in these notes. The file /boot/grub/grub.cfg is the primary configuration for GRUB. However, it should not be directly modified. There is a simpler file, /etc/default/grub that is available to be edited to make changes to GRUB's configuration. Most distributions provide a program to apply these changes to the /boot/grub/grub.cfg . On Debian-based systems it is update-grub . On RHEL-based systems, it is grub2-mkconfig . GRUB is powerful bootloader that provides options for recovering damaged systems. Upon being given control of the system by BIOS, GRUB will load its configuration and display a splash screen, allowing you to choose which OS kernel to boot. From this screen, a command-line can also be brought up to allow you to modify GRUB's configuration on the fly, or boot an OS kernel that the GRUB splash screen does not know about. Changes made to the configuration in the command line are not saved, however. If you want these changes to persist, you must modify the grub.cfg file as described before. The command line also allows you to pass command line arguments to the kernel when booting. This can allow you to boot to single-user mode (like Windows Safe mode) or set init=/bin/bash to only load a bash terminal. This provides you with options to recover a damaged system. Another note on the GRUB menu as it pertains to updates. When updating kernels, the new kernel is typically installed alongside the old one, allowing you to boot the old kernel in the event that problems occur with the new kernel. These kernel options will all become available in GRUB, which can pollute the menu. However, if the default kernel is not working, you can try selecting an old one to recover the device. The Kernel Takes Control Once the bootloader loads the kernel, it can begin to take control of the hardware. Older Linux kernels start with the init (when talking about init , people are often talking about sysvinit ) process, while newer kernels utilize systemd . The main differences are with how services are handled, and how \"run-levels\" are described. The init process uses run-levels from 0 to 6: - 0: shut down - 1 or S: single-user mode - 2 to 5: networking support - 6: reboot Entering level 0 or 6 immediately triggers a shutdown or reboot, respectively. Level 1 permits root access to the system, but does not require a password to do so. This led to the creation of level S to require a password to enter single-user mode. Levels 2 and 3 are most commonly used. Level 4 is almost never used. Level 5 is used for window login processes. The /etc/inittab file tells init what to do at each run level. systemd uses \"targets\" in place of run-levels. The configuration for targets are stored in .target files. Below are some systemd targets and their roughly equivalent run level. - poweroff.target (0) - halt the system - rescure.target (1, S) - single user mode - multi-user.target (2, 4) - user-defined, site-specific run levels. By default, 2 and 4 are the same as 3 on init systems - multi-user.target (3) - multi-user, non-graphical. Allows mutliple users to login to consoles or via network - graphical.target (5) - multi-user, graphical. Adds GUI login to multi-user.target - reboot.target (6) - reboot the system - emergency.target - emergency shell login We'll talk more about systemd vs. init in a later week when we discuss services. Briefly, systemd creates a stronger coupling between the kernel and the services (daemons) it manages. This increases the number of responsibilities a single daemon has and makes daemons more dependent on each other, but has also made managing services much easier. Most of the debate about systemd is based on adhering to a design philosophy, rather than actual functionality. systemd violates the UNIX design principle of simple software that does one thing well, as systemd handles system initialization, but then goes on to manage daemons, handle logging, etc. systemd handles many tasks. Pertaining to differences in startup, sysvinit starts daemons using shell-scripts on start up. systemd loads configuration files to start daemons. Shutting Down The shutdown command can be used to handle all shutdown-adjacent operations. Since UNIX and Linux are designed as multi-user, time-sharing systems, the default is to delay before a shutdown. Even on desktop distributions, when you click the shutdown option from the GUI it pops a window stating that the system will shut down in 60 seconds (at least, that is the default on many distros). This is done to provide other users a warning to save their work and exit before the system powers off. shutdown -h calls the halt command to power the system off completely. shutdown -r calls the reboot command to halt the system and immediately bring it back online. There are some variations in the command switches, depending on the system. shutdown --help should provide the list of options on a particular system you are working on. Access Control While some system calls have access control managed by the kernel, most access control is managed by the file system. Every file and directory has a user and a group that owns it. Each user belongs to a primary group of which they are the only member, guaranteeing each file will have a group owner. Groups are stored in the /etc/group file, but may be overwritten by a network NIS or LDAP server. In addition to owners, files have permissions for what it's user owner, group owner, and all other users can do with it. These permissions are read (r), write (w), and execute (x). The -l flag for ls can be used to display a file's permissions, user owner, and group owner. When a user starts a process, they own the process and can send the process signals and reduce its priority (but not raise it). File ownership and processes will be discussed more in detail in later weeks. The Root Account The root user is the equivalent to the Windows local Administrator account; it has total control and permission over the system. The root account has a user ID (UID) of 0. While you could create other accounts named root, rename to root account, and/or create other accounts with the UID 0, you should never do this for what are hopefully obvious reasons. The book provides plenty of background and alternatives, but here are the basics of modern root access: Logging in as the root user to perform tasks should be avoided whenever possible. There is no record of who logged in as root or what commands they executed. Allowing root logins should be disabled, except for direct access to the machine console. The root password should still be robust (follow modern requirements for password strength, as these change as technology advances). Using su to temporarily login as root is insufficient. It will log who logged in as root, but not what they did. Access to root privileges should be handled using sudo . The list of users who can run commands with sudo , and which commands they can run, are stored in the /etc/sudoers Advantages of sudo : Each user has their own \"profile\" for sudo, which specifies exactly which commands they can run as root There is a log of any attempt to run a command using sudo There is a log of every command run using sudo The user must provide their own password to run a command using sudo , increasing the likelihood the user is who they say they are Edits to the /etc/sudoers file should be done using the visudo command to ensure that the file is not being edited by someone else at the same time, and that the file has correct syntax before saving it. Saving an incorrect file may mean that you are unable to use sudo after the fact to fix it. Sudo-users or Pseudo-Users? Pseudo-users are users that are created by the system to carry out certain tasks, or to run certain processes. You should take care to ensure that these fake accounts do not become a security liability: Modify /etc/shadow to replace their password with a star (*). This prevents logging in as the user. Set their shell to /bin/false or /bin/nologin to ensure that a shell session cannot be started as that user.","title":"Week 4"},{"location":"NET264/week4/#net264-unixlinux-system-administration-week-4","text":"This week covers Chapters 2 and 3 of the 5th edition book, and Chapters 3 and 4 of the 4th edition book. The topics are Booting, Shut Down, Access Control, and Root elevation. Some details in the 4th edition book are out of date. Consult these notes for updates.","title":"NET264 - Unix/Linux System Administration: Week 4"},{"location":"NET264/week4/#booting","text":"The boot process controls how the OS takes control of the computer's hardware when the computer is powered on. The boot process has evolved over the years, but in general it starts with a bootstrapping step, where the computer is powered on, and the hardware is responsible for finding and starting the operating system. Once the OS is loaded into memory, it must start the necessary programs, DLLs, and drivers to interact with the hardware. Finally, the OS can launch its login and hand control over to the user.","title":"Booting"},{"location":"NET264/week4/#bootstrapping","text":"At this phase, the computer is highly vulnerable as it is dependent on its hardware to be functional, its configurations to be correct, and for the OS files to be uncorrupted and accessible. Despite the critical nature of bootstrapping and startup, sysadmins often have little control over the process. It is instead controlled by the manufacturer of the hardware. Some machines are purpose built to run UNIX or another proprietary operating system. Therefore, booting the targeted OS is simple. However, nowadays, many machines are PCs, which use BIOS (basic input/output system) or UEFI (unified extensible firmware interface). UEFI was designed to replace BIOS, and is the most commonly now. Colloquially, many people still call UEFI \"BIOS\", which is the convention I will follow here. If a remark pertains only to the original BIOS, it will be labeled as \"legacy BIOS\". Notes about UEFI will specifically refer to UEFI. Both are similar in that they are a very basic \"operating system\" that is installed on the computer's ROM (read-only memory). This is typically installed on the motherboard. The first thing that occurs when a computer's power switch is activated is the loading of BIOS to the computer's main memory to be executed on the CPU. BIOS's first task is the power-on self test (POST), where the system hardware is checked before loading the OS. BIOS allows you to set which devices to boot from, and in which order. BIOS will check for the existence of each device until it finds one to boot from. Legacy BIOS reads the first block of the device for the MBR (master boot record) to determine which partition to load the operating system bootloader from. UEFI reads the GUID Partition Table (GPT, no relation to the LLM) to determine where to boot. It does this by finding the special EFI partition, which has a special GUID (globally unique identifier). The EFI partition contains the GPT and the system bootloader files. At this point, the OS bootloader takes over and BIOS's job is done.","title":"Bootstrapping"},{"location":"NET264/week4/#grub","text":"Grand Unified Bootloader (GRUB) is the primary bootloader used by UNIX/Linux systems. Most distros use GRUB 2 now ( this contradicts a dated statement in the 4th edition book ), which is the successor to legacy GRUB. The primary differences are in the format and location of the configuration files, and that GRUB 2 allows the boot screen to be customized. From here on, GRUB means GRUB 2, and legacy GRUB will not be discussed in these notes. The file /boot/grub/grub.cfg is the primary configuration for GRUB. However, it should not be directly modified. There is a simpler file, /etc/default/grub that is available to be edited to make changes to GRUB's configuration. Most distributions provide a program to apply these changes to the /boot/grub/grub.cfg . On Debian-based systems it is update-grub . On RHEL-based systems, it is grub2-mkconfig . GRUB is powerful bootloader that provides options for recovering damaged systems. Upon being given control of the system by BIOS, GRUB will load its configuration and display a splash screen, allowing you to choose which OS kernel to boot. From this screen, a command-line can also be brought up to allow you to modify GRUB's configuration on the fly, or boot an OS kernel that the GRUB splash screen does not know about. Changes made to the configuration in the command line are not saved, however. If you want these changes to persist, you must modify the grub.cfg file as described before. The command line also allows you to pass command line arguments to the kernel when booting. This can allow you to boot to single-user mode (like Windows Safe mode) or set init=/bin/bash to only load a bash terminal. This provides you with options to recover a damaged system. Another note on the GRUB menu as it pertains to updates. When updating kernels, the new kernel is typically installed alongside the old one, allowing you to boot the old kernel in the event that problems occur with the new kernel. These kernel options will all become available in GRUB, which can pollute the menu. However, if the default kernel is not working, you can try selecting an old one to recover the device.","title":"GRUB"},{"location":"NET264/week4/#the-kernel-takes-control","text":"Once the bootloader loads the kernel, it can begin to take control of the hardware. Older Linux kernels start with the init (when talking about init , people are often talking about sysvinit ) process, while newer kernels utilize systemd . The main differences are with how services are handled, and how \"run-levels\" are described. The init process uses run-levels from 0 to 6: - 0: shut down - 1 or S: single-user mode - 2 to 5: networking support - 6: reboot Entering level 0 or 6 immediately triggers a shutdown or reboot, respectively. Level 1 permits root access to the system, but does not require a password to do so. This led to the creation of level S to require a password to enter single-user mode. Levels 2 and 3 are most commonly used. Level 4 is almost never used. Level 5 is used for window login processes. The /etc/inittab file tells init what to do at each run level. systemd uses \"targets\" in place of run-levels. The configuration for targets are stored in .target files. Below are some systemd targets and their roughly equivalent run level. - poweroff.target (0) - halt the system - rescure.target (1, S) - single user mode - multi-user.target (2, 4) - user-defined, site-specific run levels. By default, 2 and 4 are the same as 3 on init systems - multi-user.target (3) - multi-user, non-graphical. Allows mutliple users to login to consoles or via network - graphical.target (5) - multi-user, graphical. Adds GUI login to multi-user.target - reboot.target (6) - reboot the system - emergency.target - emergency shell login We'll talk more about systemd vs. init in a later week when we discuss services. Briefly, systemd creates a stronger coupling between the kernel and the services (daemons) it manages. This increases the number of responsibilities a single daemon has and makes daemons more dependent on each other, but has also made managing services much easier. Most of the debate about systemd is based on adhering to a design philosophy, rather than actual functionality. systemd violates the UNIX design principle of simple software that does one thing well, as systemd handles system initialization, but then goes on to manage daemons, handle logging, etc. systemd handles many tasks. Pertaining to differences in startup, sysvinit starts daemons using shell-scripts on start up. systemd loads configuration files to start daemons.","title":"The Kernel Takes Control"},{"location":"NET264/week4/#shutting-down","text":"The shutdown command can be used to handle all shutdown-adjacent operations. Since UNIX and Linux are designed as multi-user, time-sharing systems, the default is to delay before a shutdown. Even on desktop distributions, when you click the shutdown option from the GUI it pops a window stating that the system will shut down in 60 seconds (at least, that is the default on many distros). This is done to provide other users a warning to save their work and exit before the system powers off. shutdown -h calls the halt command to power the system off completely. shutdown -r calls the reboot command to halt the system and immediately bring it back online. There are some variations in the command switches, depending on the system. shutdown --help should provide the list of options on a particular system you are working on.","title":"Shutting Down"},{"location":"NET264/week4/#access-control","text":"While some system calls have access control managed by the kernel, most access control is managed by the file system. Every file and directory has a user and a group that owns it. Each user belongs to a primary group of which they are the only member, guaranteeing each file will have a group owner. Groups are stored in the /etc/group file, but may be overwritten by a network NIS or LDAP server. In addition to owners, files have permissions for what it's user owner, group owner, and all other users can do with it. These permissions are read (r), write (w), and execute (x). The -l flag for ls can be used to display a file's permissions, user owner, and group owner. When a user starts a process, they own the process and can send the process signals and reduce its priority (but not raise it). File ownership and processes will be discussed more in detail in later weeks.","title":"Access Control"},{"location":"NET264/week4/#the-root-account","text":"The root user is the equivalent to the Windows local Administrator account; it has total control and permission over the system. The root account has a user ID (UID) of 0. While you could create other accounts named root, rename to root account, and/or create other accounts with the UID 0, you should never do this for what are hopefully obvious reasons. The book provides plenty of background and alternatives, but here are the basics of modern root access: Logging in as the root user to perform tasks should be avoided whenever possible. There is no record of who logged in as root or what commands they executed. Allowing root logins should be disabled, except for direct access to the machine console. The root password should still be robust (follow modern requirements for password strength, as these change as technology advances). Using su to temporarily login as root is insufficient. It will log who logged in as root, but not what they did. Access to root privileges should be handled using sudo . The list of users who can run commands with sudo , and which commands they can run, are stored in the /etc/sudoers Advantages of sudo : Each user has their own \"profile\" for sudo, which specifies exactly which commands they can run as root There is a log of any attempt to run a command using sudo There is a log of every command run using sudo The user must provide their own password to run a command using sudo , increasing the likelihood the user is who they say they are Edits to the /etc/sudoers file should be done using the visudo command to ensure that the file is not being edited by someone else at the same time, and that the file has correct syntax before saving it. Saving an incorrect file may mean that you are unable to use sudo after the fact to fix it.","title":"The Root Account"},{"location":"NET264/week4/#sudo-users-or-pseudo-users","text":"Pseudo-users are users that are created by the system to carry out certain tasks, or to run certain processes. You should take care to ensure that these fake accounts do not become a security liability: Modify /etc/shadow to replace their password with a star (*). This prevents logging in as the user. Set their shell to /bin/false or /bin/nologin to ensure that a shell session cannot be started as that user.","title":"Sudo-users or Pseudo-Users?"}]}