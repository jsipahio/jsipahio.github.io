{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"John Sipahioglu Instructor, Computer Science - Stark State College PhD Student, Kent State University Eventually, I will clean up this site to have all my info. For now, it has info sheets/\"eBooks\" for my courses.","title":"Home"},{"location":"#john-sipahioglu","text":"","title":"John Sipahioglu"},{"location":"#instructor-computer-science-stark-state-college","text":"","title":"Instructor, Computer Science - Stark State College"},{"location":"#phd-student-kent-state-university","text":"Eventually, I will clean up this site to have all my info. For now, it has info sheets/\"eBooks\" for my courses.","title":"PhD Student, Kent State University"},{"location":"NET264/week1/","text":"NET264 - Unix/Linux System Administration: Week 1 Introduction Welcome to NET264 - Unix/Linux System Administration. The prerequisite to this course is NET220 - Unix/Linux Operating Environments, so you are expected to have some familiarity with Linux systems at this point. In this first week, we'll focus on the history of UNIX/Linux, discussing the duties for a Linux Network Administrator, and some information about man pages and getting help with Linux. Finally, we'll cover some of the installation options you can use to a Linux environment for this class. History of System Adminstrators For those of you reading the online 4th edition of the book, the section \"A Brief History of System Administration\" can be found after the Index, in the top level of the book's table of contents. The IBM 701 computer was completed in 1952, and was the first commercial computer system. It was quickly replaced 2 years later by the 704, which was totally incompatible with instructions written for the 701, as it used different word-sizes and added floating point arithmetic. The early operators of these systems layed the foundation for future SysAdmins. The informal meetings between these operators would lead to the IBM funded SHARE group, where operators would share experiences and software. SHARE continues to exist today (https://www.share.org/). These early computer systems were large and expensive, and only really designed to support a single user and task. Therefore, companies would only invest in one if they had a task that was so large and important that it was worth the expense of purchasing and maintaining it. At this point in time, the people in charge of the computers were viewed more as operators than administrators. This began to change in the 1960s, when research into designing multi-user computer systems began. One of the first attempts to develop such an operating system, Multics, was abandoned by the major research group AT&T Bell Labs after 5 years of failed development. Some remnants of this group, Ken Thompson, Rudd Canaday, and Dennis Ritchie, began working on a new system inspried by Multrics, but redesigned it to eliminate many of the pitfalls experienced when developing Multics. This system became known as UNIX, and was originally a small, single-user system. By 1971, this system began implementing many of the commands we still see today in modern Linux, such as as , cat , chdir (now cd ), chmod , chown , cmp , cp , date , and du . By 1973, the advant of the C programming language and idea of piping led to the increase in power of the UNIX operating system, along with a usage and design philosophy: - write programs that do one thing well - write programs that work together - write programs that handle text streams as a universal interface Over the next 15 or so years, the use of UNIX repidly expanded. Anti-trust rulings prevented AT&T from selling UNIX, forcing it to license it to other entities instead. One of the most important recipients of UNIX was Professor Robert Fabry at Cal Berkeley, leading the the development of Berkeley UNIX, later to be named the Berkeley Software Distribution (BSD), one of the OS's most famous distribution. Eventually, this development received funding from the US Defense Advanced Research Project Agency (DARPA). However, when AT&T was broken up do to later anti-trust rulings, it was then able to develop its own commercial distributions of UNIX and sell them. Eventually, this led to them suing Berkeley Sofware Design Inc. (BSDI) for stealing it's code. After 2 years, they were able to get a grand total of 3 out of over 18,000 files removed from BSD. However, this uncertainty led to many entities using UNIX to jump ship to Microsoft NT and Windows to avoid being at the mercy of AT&T. Meanwhile, in 1987, Professor Andrew S. Tanenbaum had released a simplified UNIX distribution called MINIX (mini-UNIX) with his book Operating Systems: Design and Implementation . This book and OS led to Linux Torvalds' development of the Linux kernel. The effect of this is that many UNIX SysAdmins shifted to learning Windows in the mid-1990s, to avoid losing their jobs due to their old skills no longer being relevant. As the dust cleared from the late 90s, it was clear that Windows was going to dominate the computing landscape. However, Linux continued to persist, and it became apparent that businesses would have to use both operating systems. Linux servers have much lower total cost of ownership (TCO) than Windows. Sys Admin Roles and Responsibilities The rest of the material in this week is from Chapter 1 of the textbook. If you are using the eBook on Safari, Chapter 1 is in Section One: Basic Administration. While system administrators are not software developers, one of thier major roles is scripting, which will be covered in the next two weeks. Here, we'll briefly discuss the other roles of SysAdmins: - Provisioning Accounts: This inclues adding new accounts for new users, deleting/deactivating accounts for people who no longer need them, and handling other account issues. - Performing Backups: A boring but critical part of being a SysAdmin. There is nothing exciting about backing up server drives, but are critical to ensuring business operations can continue if a server encounters drive damage. - Installing/Updating Software: New software must be tested before being distributed. Many companies disallow general employees from installing their own software, therefore, SysAdmins must also manage the distribution of new software. Additionally, SysAdmins must manage the updating of software, ensuring that new versions do not cause issues, or quickly distributing new updates that fix a security vulnerability. - System Monitoring: SysAdmins must keep track of the system at all times. Users generally will not notice or report minor issues. There are many software solutions for system monitoring that can aid in this task. - Troubleshooting: Admins must handle the process of diagnosing and fixing issues with the system. - Maintaining Local Documentation: Software and systems are configured to meet business needs, meaning additional documentation beyond what is provided by the software/system authors is required. - Monitoring Security: The SysAdmin is responsible for implementing the company security policy, and ensuring it is followed. They must also update the policy when neccessary to reflect changes, as software systems are constantly evolving. SysAdmins must also perform security audits to ensure that systems are not compromised. - Fire Fighting: Hopefully not literally... SysAdmins are responsible for assisting company employees with their computer issues, as well as handling major server failures. Manual Pages These are often abbreviated as \"man pages\", as the command man is used to view them. These are online documentation included with the system or software distribution, and provide information about the command/driver/file format/etc. These do not provide answers to general questions about how the software is working, but provide basic information on what the software does, and how to use it. The man command followed by the name of the command/file/etc. that you want information on is used to lookup the manual for that resource. The PAGER environment variable contains the program that is used to display the man page ( more , less , etc.). The section of the manual you want to view can be passed before the title of the resource to view the manual of. Most commands provide a one-line description. The -k followed by a keyword to search for will return all commands that contain the specified keyword in this description. For example: man -k copy asn1_copy_node (3) - API function bcopy (3) - copy byte sequence copy_file_range (2) - Copy a range of data from one file to another copysign (3) - copy sign of a number copysignf (3) - copy sign of a number copysignl (3) - copy sign of a number cp (1) - copy files and directories # ... many more results truncated Man pages are typically stored in directories under /usr/share/man/ . These pages are typically compressed using the gzip compression format, and decompressed when requested. The default search path(s) is(are) stored in the environment variable MANPATH , and can be checked using the manpath command. Beyond the included documentation installed with the system and software, much of the information can be found on the internet. Nowadays, pretty much all vendors provide documentation on their websites. Additionally, community forums exist where users and SysAdmins share problems and solutions. Requests for Comments (RFC) provide definitieve information on internet protocols and procedures. While they are very technical, anyone wanting to know the exact information about a protocol should read the associated RFC. Distros A distribution (often abbreviated to \"distro\") is the Linux kernel (the core operating system) bundled with additional software. All distros aim to provide simple installation, a package manager, and, in the case of desktop distros, some form of GUI and desktop. Many server installations do not ship with a GUI, and instead only run in terminal mode. When looking at distros for business use, it's important to consider the lifetime of the distribution, the vendor support, security update schedule, and if common software is compatible/frequently updated. There is a Linux family tree, of sorts, where there are few core distros that branch from the original kernel. For example, Debian GNU/Linux is one of the older Linux distributions. From Debian, distros including Ubuntu, Kali, and Knoppix are built off of it. Ubuntu is often called a \"cleaned-up\" version of Debian, and offers a more streamlined user experience. Furthermore, Linux Mint and Pop!_OS are built off of Ubuntu (and, by extension, also built off of Debian). Mint is often recommended as a first Linux distro to Windows users due to its simplicity and similarity to Windows' GUI. To provide a breadth of Linux distros, the book will sometimes discuss specifics of Ubuntu (Debian-based), openSUSE (SUSE based), Red Hat (RHEL-based). To provide details of operating systems more directly built off of UNIX, Solaris, HP-UX, and AIX will be discussed. Installation Options As you might imagine, you will need your own Linux environment for this class. There are several options to do so, which we'll discuss here: 1. Virtual Machine: This is the easiest and least intrusive option. One thing to note is that you may need to go into your computer's BIOS/UEFI settings and enable virtualization on your CPU. Then, you can install a hypervisor. If you have a professional or above version of Windows, you can enable Hyper-V by going to \"Turn Windows features on or off\". You can also install VMWare Workstation on Windows or VMWare Fusion on mac. However, I have heard that, while free, getting Broadcom (the new owner of VMWare) to provide the installer can be challenging. An option that works across operating systems is Oracle VirtualBox. This is a free and open source hypervisor that works on Windows, Mac, and Linux. Your \"host OS\" is the operating system installed on your computer, so make sure to choose the installer for Windows if you are on Windows, and Mac if you are on Mac. To install Linux, you will need to download the .iso file from the vendor (in this case, Ubuntu). For this class, you should use a desktop version. To create a new VM, select the correct option to create a new VM for your hypervisor. The steps will be slightly different, but you will need to tell it how many CPU cores and how much RAM to dedicate to the VM, and select a location to create the Virtual Hard Disk. You can do this on either your main computer's storage drive, or on an external hard drive you USB thumbdrive. Using a thumbdrive or external drive will be slower, but is an option if you do not have very much storage space remaining on your computer. 2. Separate Drive: You can partition and install Linux on a separate hard drive, either by installing an additional hard disk or SSD into your computer if it allows it, or an external hard drive. If it is an external drive, you'll need to make it bootable. A SATA connected drive will be detected as bootable as soon as you install the OS. To install Linux, you will need to download the .iso file from the vendor, in this case, Ubuntu desktop. Next, you will need a USB flash drive large enough (8GB for newer Ubuntu versions, some older ones may work with 4GB) to store the Linux ISO. Rufus is a commonly used tool for this purpose. Here are the instructions from Ubuntu (https://documentation.ubuntu.com/desktop/en/latest/how-to/create-a-bootable-usb-stick/#on-windows) to create a bootable USB on Windows. There are instructions for Mac below the Windows Section. 3. Dual-Boot on Same Drive : This is the most advanced option. You'll need to go into Windows drive management and shrink your Windows partition. 20GB is the minimum recommended amount of drive space for installing Ubuntu. Once you shrink your Windows partition, you'll need to create a bootable CD or USB (same process as option 2) to install Linux. You then need to restart your computer, enter BIOS/UEFI, and boot from the USB. Then, when using the Linux installer, you should see the unallocated space created by shrinking your Windows partition. Use all this space for Linux. Note: A VM is by far the easiest and cheapest option. I would only recommend the other options if you would like to use Linux as an alternate OS in your day-to-day life. Anything for this class, or even any testing you do in the future, can be done using a VM. Installing Linux Once you get to this point, the steps to install Linux will be the same. You will need to configure the partition table for your Linux system. The first 512 bytes should be dedicated for EFI Boot. The next section is the root (signified by a single forward slash / ). Use \"ext4\" as the file system. I'd recommend dedicating at least 4 GB of RAM to it, but 8 would be better, if you have enough storage. If your VM or computer (depending on the install option) has more than 8 GB of RAM, you can dedicate the rest of the free space to the /home directory. This should also use the ext4 file system. If you have less than 8GB of RAM, I'd recommend dedicating all but 4GB of the remaining free space to /home . After home, add an additional \"swap\" partition. This will provide the OS with virtual RAM to use if it runs out of main memory. From here, you can pretty much just click through all the boxes and the installation will proceed. If you are not using a VM, you may want to consider going back into BIOS/UEFI settings and making your Linux drive/partition the first option. This way, the GRUB menu will appear when you start your system and you can choose whether to boot Linux, or your original OS. Otherwise, you will have to go into BIOS/UEFI each time you start the system to tell it to boot Linux.","title":"Week 1"},{"location":"NET264/week1/#net264-unixlinux-system-administration-week-1","text":"","title":"NET264 - Unix/Linux System Administration: Week 1"},{"location":"NET264/week1/#introduction","text":"Welcome to NET264 - Unix/Linux System Administration. The prerequisite to this course is NET220 - Unix/Linux Operating Environments, so you are expected to have some familiarity with Linux systems at this point. In this first week, we'll focus on the history of UNIX/Linux, discussing the duties for a Linux Network Administrator, and some information about man pages and getting help with Linux. Finally, we'll cover some of the installation options you can use to a Linux environment for this class.","title":"Introduction"},{"location":"NET264/week1/#history-of-system-adminstrators","text":"For those of you reading the online 4th edition of the book, the section \"A Brief History of System Administration\" can be found after the Index, in the top level of the book's table of contents. The IBM 701 computer was completed in 1952, and was the first commercial computer system. It was quickly replaced 2 years later by the 704, which was totally incompatible with instructions written for the 701, as it used different word-sizes and added floating point arithmetic. The early operators of these systems layed the foundation for future SysAdmins. The informal meetings between these operators would lead to the IBM funded SHARE group, where operators would share experiences and software. SHARE continues to exist today (https://www.share.org/). These early computer systems were large and expensive, and only really designed to support a single user and task. Therefore, companies would only invest in one if they had a task that was so large and important that it was worth the expense of purchasing and maintaining it. At this point in time, the people in charge of the computers were viewed more as operators than administrators. This began to change in the 1960s, when research into designing multi-user computer systems began. One of the first attempts to develop such an operating system, Multics, was abandoned by the major research group AT&T Bell Labs after 5 years of failed development. Some remnants of this group, Ken Thompson, Rudd Canaday, and Dennis Ritchie, began working on a new system inspried by Multrics, but redesigned it to eliminate many of the pitfalls experienced when developing Multics. This system became known as UNIX, and was originally a small, single-user system. By 1971, this system began implementing many of the commands we still see today in modern Linux, such as as , cat , chdir (now cd ), chmod , chown , cmp , cp , date , and du . By 1973, the advant of the C programming language and idea of piping led to the increase in power of the UNIX operating system, along with a usage and design philosophy: - write programs that do one thing well - write programs that work together - write programs that handle text streams as a universal interface Over the next 15 or so years, the use of UNIX repidly expanded. Anti-trust rulings prevented AT&T from selling UNIX, forcing it to license it to other entities instead. One of the most important recipients of UNIX was Professor Robert Fabry at Cal Berkeley, leading the the development of Berkeley UNIX, later to be named the Berkeley Software Distribution (BSD), one of the OS's most famous distribution. Eventually, this development received funding from the US Defense Advanced Research Project Agency (DARPA). However, when AT&T was broken up do to later anti-trust rulings, it was then able to develop its own commercial distributions of UNIX and sell them. Eventually, this led to them suing Berkeley Sofware Design Inc. (BSDI) for stealing it's code. After 2 years, they were able to get a grand total of 3 out of over 18,000 files removed from BSD. However, this uncertainty led to many entities using UNIX to jump ship to Microsoft NT and Windows to avoid being at the mercy of AT&T. Meanwhile, in 1987, Professor Andrew S. Tanenbaum had released a simplified UNIX distribution called MINIX (mini-UNIX) with his book Operating Systems: Design and Implementation . This book and OS led to Linux Torvalds' development of the Linux kernel. The effect of this is that many UNIX SysAdmins shifted to learning Windows in the mid-1990s, to avoid losing their jobs due to their old skills no longer being relevant. As the dust cleared from the late 90s, it was clear that Windows was going to dominate the computing landscape. However, Linux continued to persist, and it became apparent that businesses would have to use both operating systems. Linux servers have much lower total cost of ownership (TCO) than Windows.","title":"History of System Adminstrators"},{"location":"NET264/week1/#sys-admin-roles-and-responsibilities","text":"The rest of the material in this week is from Chapter 1 of the textbook. If you are using the eBook on Safari, Chapter 1 is in Section One: Basic Administration. While system administrators are not software developers, one of thier major roles is scripting, which will be covered in the next two weeks. Here, we'll briefly discuss the other roles of SysAdmins: - Provisioning Accounts: This inclues adding new accounts for new users, deleting/deactivating accounts for people who no longer need them, and handling other account issues. - Performing Backups: A boring but critical part of being a SysAdmin. There is nothing exciting about backing up server drives, but are critical to ensuring business operations can continue if a server encounters drive damage. - Installing/Updating Software: New software must be tested before being distributed. Many companies disallow general employees from installing their own software, therefore, SysAdmins must also manage the distribution of new software. Additionally, SysAdmins must manage the updating of software, ensuring that new versions do not cause issues, or quickly distributing new updates that fix a security vulnerability. - System Monitoring: SysAdmins must keep track of the system at all times. Users generally will not notice or report minor issues. There are many software solutions for system monitoring that can aid in this task. - Troubleshooting: Admins must handle the process of diagnosing and fixing issues with the system. - Maintaining Local Documentation: Software and systems are configured to meet business needs, meaning additional documentation beyond what is provided by the software/system authors is required. - Monitoring Security: The SysAdmin is responsible for implementing the company security policy, and ensuring it is followed. They must also update the policy when neccessary to reflect changes, as software systems are constantly evolving. SysAdmins must also perform security audits to ensure that systems are not compromised. - Fire Fighting: Hopefully not literally... SysAdmins are responsible for assisting company employees with their computer issues, as well as handling major server failures.","title":"Sys Admin Roles and Responsibilities"},{"location":"NET264/week1/#manual-pages","text":"These are often abbreviated as \"man pages\", as the command man is used to view them. These are online documentation included with the system or software distribution, and provide information about the command/driver/file format/etc. These do not provide answers to general questions about how the software is working, but provide basic information on what the software does, and how to use it. The man command followed by the name of the command/file/etc. that you want information on is used to lookup the manual for that resource. The PAGER environment variable contains the program that is used to display the man page ( more , less , etc.). The section of the manual you want to view can be passed before the title of the resource to view the manual of. Most commands provide a one-line description. The -k followed by a keyword to search for will return all commands that contain the specified keyword in this description. For example: man -k copy asn1_copy_node (3) - API function bcopy (3) - copy byte sequence copy_file_range (2) - Copy a range of data from one file to another copysign (3) - copy sign of a number copysignf (3) - copy sign of a number copysignl (3) - copy sign of a number cp (1) - copy files and directories # ... many more results truncated Man pages are typically stored in directories under /usr/share/man/ . These pages are typically compressed using the gzip compression format, and decompressed when requested. The default search path(s) is(are) stored in the environment variable MANPATH , and can be checked using the manpath command. Beyond the included documentation installed with the system and software, much of the information can be found on the internet. Nowadays, pretty much all vendors provide documentation on their websites. Additionally, community forums exist where users and SysAdmins share problems and solutions. Requests for Comments (RFC) provide definitieve information on internet protocols and procedures. While they are very technical, anyone wanting to know the exact information about a protocol should read the associated RFC.","title":"Manual Pages"},{"location":"NET264/week1/#distros","text":"A distribution (often abbreviated to \"distro\") is the Linux kernel (the core operating system) bundled with additional software. All distros aim to provide simple installation, a package manager, and, in the case of desktop distros, some form of GUI and desktop. Many server installations do not ship with a GUI, and instead only run in terminal mode. When looking at distros for business use, it's important to consider the lifetime of the distribution, the vendor support, security update schedule, and if common software is compatible/frequently updated. There is a Linux family tree, of sorts, where there are few core distros that branch from the original kernel. For example, Debian GNU/Linux is one of the older Linux distributions. From Debian, distros including Ubuntu, Kali, and Knoppix are built off of it. Ubuntu is often called a \"cleaned-up\" version of Debian, and offers a more streamlined user experience. Furthermore, Linux Mint and Pop!_OS are built off of Ubuntu (and, by extension, also built off of Debian). Mint is often recommended as a first Linux distro to Windows users due to its simplicity and similarity to Windows' GUI. To provide a breadth of Linux distros, the book will sometimes discuss specifics of Ubuntu (Debian-based), openSUSE (SUSE based), Red Hat (RHEL-based). To provide details of operating systems more directly built off of UNIX, Solaris, HP-UX, and AIX will be discussed.","title":"Distros"},{"location":"NET264/week1/#installation-options","text":"As you might imagine, you will need your own Linux environment for this class. There are several options to do so, which we'll discuss here: 1. Virtual Machine: This is the easiest and least intrusive option. One thing to note is that you may need to go into your computer's BIOS/UEFI settings and enable virtualization on your CPU. Then, you can install a hypervisor. If you have a professional or above version of Windows, you can enable Hyper-V by going to \"Turn Windows features on or off\". You can also install VMWare Workstation on Windows or VMWare Fusion on mac. However, I have heard that, while free, getting Broadcom (the new owner of VMWare) to provide the installer can be challenging. An option that works across operating systems is Oracle VirtualBox. This is a free and open source hypervisor that works on Windows, Mac, and Linux. Your \"host OS\" is the operating system installed on your computer, so make sure to choose the installer for Windows if you are on Windows, and Mac if you are on Mac. To install Linux, you will need to download the .iso file from the vendor (in this case, Ubuntu). For this class, you should use a desktop version. To create a new VM, select the correct option to create a new VM for your hypervisor. The steps will be slightly different, but you will need to tell it how many CPU cores and how much RAM to dedicate to the VM, and select a location to create the Virtual Hard Disk. You can do this on either your main computer's storage drive, or on an external hard drive you USB thumbdrive. Using a thumbdrive or external drive will be slower, but is an option if you do not have very much storage space remaining on your computer. 2. Separate Drive: You can partition and install Linux on a separate hard drive, either by installing an additional hard disk or SSD into your computer if it allows it, or an external hard drive. If it is an external drive, you'll need to make it bootable. A SATA connected drive will be detected as bootable as soon as you install the OS. To install Linux, you will need to download the .iso file from the vendor, in this case, Ubuntu desktop. Next, you will need a USB flash drive large enough (8GB for newer Ubuntu versions, some older ones may work with 4GB) to store the Linux ISO. Rufus is a commonly used tool for this purpose. Here are the instructions from Ubuntu (https://documentation.ubuntu.com/desktop/en/latest/how-to/create-a-bootable-usb-stick/#on-windows) to create a bootable USB on Windows. There are instructions for Mac below the Windows Section. 3. Dual-Boot on Same Drive : This is the most advanced option. You'll need to go into Windows drive management and shrink your Windows partition. 20GB is the minimum recommended amount of drive space for installing Ubuntu. Once you shrink your Windows partition, you'll need to create a bootable CD or USB (same process as option 2) to install Linux. You then need to restart your computer, enter BIOS/UEFI, and boot from the USB. Then, when using the Linux installer, you should see the unallocated space created by shrinking your Windows partition. Use all this space for Linux. Note: A VM is by far the easiest and cheapest option. I would only recommend the other options if you would like to use Linux as an alternate OS in your day-to-day life. Anything for this class, or even any testing you do in the future, can be done using a VM.","title":"Installation Options"},{"location":"NET264/week1/#installing-linux","text":"Once you get to this point, the steps to install Linux will be the same. You will need to configure the partition table for your Linux system. The first 512 bytes should be dedicated for EFI Boot. The next section is the root (signified by a single forward slash / ). Use \"ext4\" as the file system. I'd recommend dedicating at least 4 GB of RAM to it, but 8 would be better, if you have enough storage. If your VM or computer (depending on the install option) has more than 8 GB of RAM, you can dedicate the rest of the free space to the /home directory. This should also use the ext4 file system. If you have less than 8GB of RAM, I'd recommend dedicating all but 4GB of the remaining free space to /home . After home, add an additional \"swap\" partition. This will provide the OS with virtual RAM to use if it runs out of main memory. From here, you can pretty much just click through all the boxes and the installation will proceed. If you are not using a VM, you may want to consider going back into BIOS/UEFI settings and making your Linux drive/partition the first option. This way, the GRUB menu will appear when you start your system and you can choose whether to boot Linux, or your original OS. Otherwise, you will have to go into BIOS/UEFI each time you start the system to tell it to boot Linux.","title":"Installing Linux"}]}